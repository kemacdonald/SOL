---
title: "Age-related changes in children’s real-time American Sign Language comprehension"
author: |
  | Kyle MacDonald^1^, Todd LeMarr^1^, David Corina^2^, Virginia A. Marchman^1^, & Anne Fernald^1^
  | 1. Stanford University
  | 2. University of California Davis
bibliography: references-sol.bib
output:
  word_document:
    fig_caption: yes
    reference_docx: template_final.docx
  pdf_document:
    fig_caption: yes
    keep_tex: yes
  html_document: default
fontsize: 12pt
csl: apa6.csl
abstract: Research with children learning spoken language shows that the ability to
  link the auditory signal to objects in the world with high efficiency is critical
  to language development [@fernald2012individual]. Children learning sign language
  must use vision to process *both* linguistic information and objects in the visual
  world, creating a challenge for young learners' real-time language comprehension.  This
  cross-sectional study is the first study to explore how young learners of American
  Sign Language (ASL) comprehend signed language in real time, and explores links
  between these skills, children’s age, and vocabulary development. Native ASL-learning
  children (16-53 mos, n=29) and fluent adult signers (n=19) participated in a real-time
  language comprehension task of ASL processing efficiency. Children’s comprehension
  skills improved with age, with adult signers being most efficient. Importantly,
  children’s processing skills significantly correlated with age and vocabulary size,
  providing evidence that the ability to establish reference in real-time is linked
  meaningfully to language development. These novel findings show striking parallels
  between efficiency of language comprehension in visual language learners and children
  learning spoken languages, with both groups making impressive gains in the efficiency
  of language interpretation over the first few years of life as they progress towards
  adult-like levels of fluency.
---
\newpage

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE)
```

```{r packages}
rm(list=ls())
library(dplyr)
library(tidyr)
library(ggplot2)
library(car)
```

```{r load data}
# load summary data
ss_kids_df <- read.csv("sol_ss_kids.csv", stringsAsFactors = F)

# filter to get rt data set
ss_kids_rt_df <- filter(ss_kids_df, exclude_chance_shifter == "include")

# load graph values for making profile plot
sol_gvals_df <- read.csv("sol_gvals.csv", stringsAsFactors = F)

# load exlusions 
sol_exclusions_df <- read.csv("sol_exclusions.csv", stringsAsFactors = F)

# load filtered and labeled iChart
sol_iChart_df <- read.csv("sol_ichart_labeled_filtered.csv")
```

# Introduction 

Understanding language rapidly and accurately is central to our ability to function effectively in daily life. One fundamental component of language understanding is establishing reference during real-time language interaction by linking abstract symbols (i.e., words and signs) to concrete objects in the world [^1]. While children learning spoken languages can simultaneously attend to objects and listen to their caregivers talk, children learning American Sign Language (ASL) must rely on vision to both process linguistic information and look at objects in the visual scene. This dual functionality requires children to disengage from the source of language to seek out the named object, increasing the likelihood of a mapping error or potentially creating a situation where subsequent linguistic information is missed. However, we know relatively little about how children acquiring ASL learn to efficiently allocate visual attention in the service of language learning. In the current work, we adapt a well-established paradigm for measuring spoken language processing efficiency to be used with young children learning ASL. Next, we ask whether increased efficiency in early comprehension by native ASL learners follows a similar developmental trajectory as that of children learning spoken language. Finally, we test whether individual variation in ASL processing skills show similar concurrent relations to children’s age and vocabulary size.

[^1]: This problem is also known as the core problem of referential uncertainty [@quine1960word]: that an utterance could refer to many possible objects in the visual scene, to parts of of those objects, or even to something that is not present, creating an a priori infinitely large hypothesis space of possible word/sign meanings.

### Spoken language processing 

To follow a typical conversation, skilled listeners must rapidly apprehend meaning in combinations of words from moment to moment as the speech signal unfolds at rates of 10-15 phonemes/second. Extensive research with adults using online measures[^2] shows that skilled listeners can identify spoken words before their acoustic offset, evaluating hypotheses about word identity incrementally based on what they have heard up to that moment, typically within 150 ms of word onset [@marslen1989accessing]. Moreover, adults are adept in the parallel processing of multiple streams of information, rapidly integrating the acoustic speech signal as it unfolds in time with information from the visual scene to derive intended meaning [@dahan2004continuous; @tanenhaus1995integration; @altmann1999incremental].

[^2]: Here online measures refer to measuring participants’ eye movements during language comprehension in order to provide a rapid and detailed metric for determining the target of their visual attention. 

Over the past fifteen years, research with infants and young children has incorporated the same high-resolution measures of language processing [@fernald1998rapid; @snedeker2004developing], making it possible to obtain continuous measures of speed and accuracy that enable sensitive assessment of efficiency in spoken language processing even by very young children. Using these procedures, researchers have found systematic age-related changes in the speed and accuracy of responses to familiar words [@fernald1998rapid], and that efficiency in word recognition is correlated with both individual differences in vocabulary knowledge [@fernald2001half; @zangl2005dynamics] as well as faster rates of vocabulary growth across the second year [@fernald2006picking]. While studies have also found associations between faster word recognition and more advanced linguistic development in both English [@fernald2001half; @zangl2007increasing] and Spanish [@hurtado2008does; @lew2007young], this is the first study to adapt these online processing efficiency measures to be used with children learning ASL.

### ASL processing with adults

ASL is a visual-gestural language expressed with hands, arms and face, a modality difference with potential consequences for how linguistic information is processed. In many ways, language processing appears to be parallel in spoken and manual modalities. Signers show effects of: (a) lexicality, response times to identify non-signs are slower than for actual signs [@corina1993lexical], (b) frequency, high frequency signs are recognized faster than low frequency signs [@carreiras2008lexical], and (c) phonological parameters, the sublexical units of sign – handshape, location, and movement – influence sign recognition [@corina1993lexical; @hildebrandt2002phonological; @carreiras2008lexical]. But, differences in linguistic structure and surface features of lexical forms in the spoken vs. manual modality have consequences for the efficiency with which signs are understood [@carreiras2010sign; @corina2006lexical]. Using a gating procedure, @emmorey1990lexical found that deaf participants identified monomorphemic signs after approximately 35% of the sign form had been seen; in contrast, in spoken English approximately 83% of a word must be heard before words are uniquely identified [@grosjean1980spoken].

Another line of research has explored the consequences of delayed first language acquisition for language processing, finding consistent processing advantages for early learners. For example, @mayberry1989looking had native and non-native signers complete a linguistic shadowing task and found that non-native signers expended more cognitive resources processing signs at the phonological level. Processing advantages for native signers also show up in sentence recall tasks [@mayberry1991long], grammaticality judgments [@boudreault2006grammatical], and a variety of receptive and productive tasks [@newport1990maturational]. In addition, @emmorey1990lexical found that late signers were delayed relative to native signers in isolating signs as well as in individual phonological parameters (i.e. handshape, movement, location) in lexical recognition. 

More recent work using a novel adapation of the visual world paradigm [@tanenhaus1995integration] has investigated questions about the online comprehension of sign language by measuring adult signers' eye movements as they process ASL. @lieberman2014real found that early, but not late-learners, show evidence of real-time activation of sublexical features of sign and that incremental semantic processing occurs during real-time sign comprehension. And @thompson2013lexical showed that both semantic and phonological aspects of signs affect real-time lexical processing. Thus, there is considerable evidence that signs, like spoken words, are processed incrementally by adults, and that there are substantial individual differences in adults' linguistic processing skills. At the same time, we know very little about how young ASL learners develop these critical, real-time language processing skills and whether these skills are linked to lexical development.

### Lexical development in ASL

Since the seminal work of @bellugi1979signs established that signed languages are natural human languages not derivative from spoken languages, researchers have explored the effects of a visual-manual communication system on lexical development. The upshot of this work is that acquisition of ASL in native, natural contexts follows a strikingly similar developmental path to children learning spoken language [@lillo1999modality; @mayberry2006sign]. For example, like children learning spoken languages, young signers produce first signs typically before the end of the first year and two-sign sentences by their 2nd birthday [@newport1985acquisition]. Moreover, young ASL learners show a preponderance of nouns, rather than verbs or other predicates, in the early lexicon [@anderson2002macarthur]. 

A separate body of research has investigated how children learning ASL alternate gaze between linguistic information and objects in real-world learning contexts to achieve joint attention [@waxman1997mothers]. [^3] For example, @harris1997learning found that at 18 months, deaf children frequently shifted visual attention towards their mothers during a free play interaction. More recent work by @lieberman2014learning showed that deaf children make frequent shifts in gaze during book reading in order to perceive both linguistic input and the non-linguistic context. Thus, gaze shifts are a critical aspect of sign language comprehension, but we do not have precise experimental measures of how young ASL users develop these visual attention skills. 

[^3]: Both experimental and observational data with children learning spoken language shows that joint attention facilitates language acquisition [@baldwin1995understanding; @brooks2008infant].

In sum, data on the developmental trajectories of deaf children learning signed languages has been largely confined to diary studies and small-group investigations. These studies have also overwhelmingly focused on aspects of language production, for example, the development of ASL articulatory skills [@meier1998motoric] or the appearance of specific grammatical forms [@lillo2000early]. Moreover, no prior studies have systematically investigated how young ASL learners link signs to objects during real-time sentence processing, or whether early language comprehension skills are linked to other meaningful linguistic outcomes. In the current work, we address this gap by developing the first measures of young learners' visual language comprehension skills.  

### Current study

This study will be the first to explore the early development of real-time processing of signs by very young children learning ASL. First, we adapt a well-established paradigm for measuring spoken language processing efficiency to be used with young children learning ASL. Next, we ask whether efficiency of ASL comprehension reveals similar age-related changes as that seen in previous studies with children learning spoken language. Finally, we test whether individual variation in ASL processing efficiency shows concurrent relations to children’s vocabulary size.

# Method

### Participants

```{r participants table}
ss_df1 <- ss_kids_df %>%
    group_by(age_group, gender) %>% 
    tally() %>% 
    spread(gender, n)

ss_df2 <- ss_kids_df %>%
    group_by(age_group, hearing_status_participant) %>% 
    tally() %>% 
    spread(hearing_status_participant, n)

descriptives_df <- ss_kids_df %>% group_by(age_group) %>% 
    summarise(n = n_distinct(Sub.Num),
              mean = round(mean(Months), 1), 
              min = min(Months), 
              max = max(Months)) %>% 
    left_join(ss_df1, by = "age_group") %>% 
    left_join(ss_df2, by = "age_group")

n_deaf <- sum(descriptives_df$deaf)
n_hearing <- sum(descriptives_df$hearing)
n_female <- sum(descriptives_df$f)
n_male <- sum(descriptives_df$m)
mean_age <- round(mean(ss_kids_df$Months), 1)
age_range <- range(ss_kids_df$Months)

ss_exclusions <- sol_exclusions_df %>% 
    filter(reason_excluded %in% c("age", "too few trials", "language")) %>% 
    group_by(reason_excluded) %>% 
    summarise(n_excluded = n())  
    
n_excluded <- sum(ss_exclusions$n_excluded)

# info about the target signs
min_sign_length <- 495
max_sign_length <- 1947
average_sign_length <- 1134
```

`r n_deaf` deaf and `r n_hearing` hearing children with native exposure to ASL (`r n_female` females, `r n_male` males, Mage = `r mean_age` months, range = `r age_range[1]`-`r age_range[2]` months) and 19 fluent adults were recruited from several locations by bi-cultural/bilingual researchers fluent in ASL. All children were exposed to ASL at birth from at least one fluent ASL caregiver and currently used ASL as their primary mode of communication at home. The majority of children attended a center-based early childhood education program in which ASL was the primary mode of instruction. Thus, all children in the sample had at least one deaf caregiver and were immersed in ASL from birth, both at home and in the daycare setting. An additional `r n_excluded` participants were tested, but not included in the analyses due to fussiness (n = `r ss_exclusions$n_excluded[3]`), being outside the target age range (n = `r ss_exclusions$n_excluded[1]`), or not receiving enough ASL exposure (n = `r ss_exclusions$n_excluded[2]`). For visualization purposes, children were divided into two groups using a median split by age: Younger (`r descriptives_df$age_group[1]`), Older (`r descriptives_df$age_group[2]`), but we conduct all critical statistical tests using a continuous measure of age [^4].

[^4]: See @mcclelland2015median for a discussion of the potential for increasing Type I and Type II errors through the loss of power caused by using median splits. 

```{r table, eval = F}
knitr::kable(descriptives_df, digits=2, 
             caption="Participant background information. All children were exposed to ASL from one caregiver from birth.",
             col.names = c("Age Group", "n", "Mean age", 
                           "Min age", "Max age", "Female", "Male", "Deaf", 
                           "Hearing"))
```

### Measures
 
*Parent report of vocabulary size*: Parents completed a 90-item vocabulary checklist based on the MacArthur-Bates Communicative Development Inventories [@fenson2007macarthur] and designed to be culturally and linguistically appropriate for children learning ASL. Parents completed the checklist during the visit, and vocabulary size was computed as the number of signs reported to be produced.
 
*ASL Processing*: Efficiency in online comprehension was assessed using a version of the looking-while-listening procedure (LWL) [@fernald2006picking] adapted for ASL learners, which we call the Visual Language Processing (VLP) task. Since this was the first study to measure online ASL processing efficiency in children of this age, several important modifications to the procedure were made, which we describe below.

### Apparatus

To facilitate recrutiment[^5], we created a portable version of the VLP task with stimuli presented on a 27” monitor using a Macbook Pro laptop. Video of the child’s gaze was recorded using a digital camcorder set up behind the monitor. To minimize visual distractions, children sat on their caregivers’ laps inside of a portable 5’ by 5’ tent with opaque walls. The tent reduced the potential for visual distractions to occur during the task.

[^5]: Native ASL learners are a difficult population to recruit because approximately 95% of deaf children are born to hearing parents with little prior exposure to a signed language [@mitchell2004chasing].

```{r timeline, cache=T, fig.width=6, fig.height=3, fig.cap = "Timeline of a trial on the VLP task."}
grid::grid.raster(png::readPNG("Figs/timeline.png"))
```

### Trial Structure

Figure 1 shows an example of the stimuli and the timeline of one trial in the VLP task. On each trial the child saw two images on the screen for two seconds before the signer appeared. This allowed the child to inspect both images prior to the start of the sentence. Next, children saw a still frame of the signer for one second, which gave them the opportunity to orient to the signer prior to the sentence onset. The target noun was then presented, followed by a question and hold which gave the child the opportunity to their attention to the target object.  After the hold, the signer gave neutral, positive feedback to help maintain the children's focus throughout the task. Each sentence lasted for approximately five seconds. 

### Linguistic and visual stimuli

The linguistic stimuli were designed to be comparable to those used in previous research and to allow for generalization beyond characteristics of a specific signer and 
sentence structure. To accomplish this, ASL stimuli were recorded by two different native ASL users using two different but acceptable ASL sentence structures for asking questions[^6]:

[^6]: See @neidle1998wh for a detailed discussion of the acceptability of these two question structures. It is important to point out that we first analyzed responses for the two sentence frames separately and found no significant differences between the two. Thus, all analyses we report are collapsed across the two sentence structures.

- Sentence-initial wh-phrase: “HEY! WHERE [target noun]?”
- Sentence-final wh-phrase: “HEY! [target noun] WHERE?”

Before each sentence, the signer used a hand-wave gesture commonly used in ASL discourse to initiate a linguistic utterance. This served to shift children's attention away from the images on the screen to the signer in preparation for the upcoming linguistic information. 

Four yoked pairs of eight target nouns (cat—bird, car—book, bear—doll, ball—shoe) were used. These nouns were selected such that they would be familiar to most children learning ASL at this age and have minimal phonological overlap. To prepare the stimuli, two female native ASL users recorded several tokens of each sentence, matching them closely in prosody. These candidate stimuli were then digitized, analyzed, and edited using Final Cut Pro software. The final tokens were chosen based on naturalness and prosodic comparability. The mean duration of target nouns was `r average_sign_length`  ms (range =  `r min_sign_length`-`r max_sign_length` ms). Five filler trials were interspersed among the 32 test trials (e.g. “YOU LIKE PICTURES! MORE WANT?”). Images were digitized pictures presented in fixed pairs, matched for visual salience with 3–4 tokens of each object type. Side of target picture was counterbalanced across trials.

### Coding and reliability

Children’s gaze patterns were videotaped and coded frame-by-frame, yielding a high-resolution record of eye movements aligned with target noun onset. 25% of videos were re-coded to assess coder reliability -- agreement within a single frame averaged 98% on these reliability assessments.

### Calculating linguistic processing efficiency
 
```{r RT descriptives}
shifts_df <- sol_iChart_df %>% 
    group_by(trial_types) %>% 
    summarise(count = n())

# percentage of trials no-shifts
prop_no_shifts <- round(shifts_df$count[3] / sum(shifts_df$count), 2) * 100

# center to target shifts
mean_n_ct <- round(mean(ss_kids_rt_df$C_T_count),1)
range_ct <- round(range(ss_kids_rt_df$C_T_count),1)

# analysis window
rts <- filter(sol_iChart_df, trial_types %in% c("C_T"))
analysis.window <- c(500, 2000)
```
 
*Computing critical sign onset.* Computing accuracy and RT requires defining the appropriate response window, starting at the earliest moment when there is sufficient information to shift gaze off of the central signer and initiate a looking response to the target image.  In studies of spoken language processing, critical word onset is typically identified using speech analyses software that measures the moment in the speech when the target noun begins. Because of the potential for substantial co-articulation in visual languages, it is difficult to precisely determine the moment at which a sign begins (CITATION). In order to make an initial judgment, the first and second authors, both fluent signers, viewed each stimulus sentence and achieved consensus regarding target noun onset. To validate these judgments, an additional study was conducted in which naive fluent adult signers (n = 10) made force choice decisions indicating which of two images were the referent for six tokens of each stimulus sentence. On each trial, the stimulus presented +/- three frames of video from the noun onset defined in the initial consensus judgments.  A total of 168 trials (28 sentences x 6 tokens) were presented in random order.  Final noun onset values were derived based on the earliest point in the sentence when judgments achieved 100% agreement. 
 
*Accuracy:* Correct looking is a function of the child’s tendency to shift quickly away from the central signer to the target picture in response to the target sign, and also to remain fixated on the target picture. To determine the degree to which participants fixated the appropriate picture across trials, mean proportion looking to target was calculated for each participant at each 33 ms frame from the onset of the target noun. Accuracy was defined as the mean proportion of time spent looking at the target picture out of the total time spent on either the target picture, the distracter picture, or the signer from `r analysis.window[1]` to `r analysis.window[2]` ms from target noun onset. We selected this window after looking at the distribution of children's first shifts, with the goal of maximizing the amount of meaningful looking behavior. This window includes 90% of children's first shifts off the center signer. Importantly, the VLP task includes a central signer, which functions as a central fixation point similar to adult psycholinguistic experiments. Thus children could produce four different types of responses on a given trial: (1) signer-to-target shift, (2) signer-to-distractor shift, (3) signer-to-away shift, (4) no-shift. All four trial types contribute to accuracy analyses and all 29 children were included. 

*Reaction Time:* Reaction time (RT) corresponds to the latency to shift away from the signer to the target picture, measured from the onset of the target sign. Incorrect shifts from the signer to the distractor picture were not included in the computation of mean RT. To determine whether children's initial shifts were the result of guessing, we applied a Bayesian latent mixture guessing model implemented in JAGS [@plummer2003jags]. In this model, data are assumed to be generated by two different processes (guessing and knowledge) which have different probabilities of success, with the guessing group having a probability of 0.5 and the knowledge group having a probability greater than 0.5. The group membership of each participant is a latent variable that we infer based on their proportion of correct shifts to the target picture relative to the overall proportion of correct shifts across all participants (see @lee2013bayesian for a discussion of this modeling approach). Data from 5 children were excluded because more than 50% of their posterior mass indicated a guessing strategy with relatively little uncertainty -- posterior probabilities of 0.99, 0.78, 0.98, 0.98, and 0.94 respectively -- suggesting that RTs for these children are not a meaningful measure of their language processing skill [^7]. Thus, only signer-to-target shifts for 24 children were included in the RT analyses.

[^7]: Mean proportion accuracy scores for these participants were: 0.54, 0.58, 0.42, 0.29, 0.33.

Within children, shifts that occurred prior to `r analysis.window[1]` ms from noun onset were excluded because it is likely that these shifts were initiated before the child had enough time to process sufficient linguistic input and to mobilize an eye movement; shifts that occurred after `r analysis.window[2]` ms were excluded because these delayed looks are less likely to reflect a response to the target sign (see @fernald2001half). In addition, `r prop_no_shifts`% of trials were excluded because children never shifted off of the signer. Since children vary in the likelihood that they will shift on a given trial, mean RTs are based on different numbers of trials across participants (M = `r mean_n_ct` trials, range = `r range_ct[1]`—`r range_ct[2]`). 

```{r distribution of RTs, fig.height=4, fig.width=8}
rt_dist <- qplot(rts$RT) +
    geom_vline(x=analysis.window[1], col="red", lwd=1.5) +
    geom_vline(x=analysis.window[2], col="red", lwd=1.5) +
    xlab("Reaction Time in ms") +
    ylab("Number of Trials") +
    annotate("text", x = 2350, y = 30, 
              label = "Analysis Window \n (90% RTs)", size = 5) +
    theme_bw() +
    theme(axis.title.x = element_text(colour="grey30",size=18,
                                      angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.title.y = element_text(colour="grey30",size=18, hjust=0.5,vjust=0.5,face="plain"),
          axis.text.x = element_text(colour="grey20",size=14,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.text.y = element_text(colour="grey20",size=14,
                                     angle=0,hjust=0.5,vjust=0,face="plain"))
```

# Results 

```{r correlations age}
# correlations age
cor_acc_age <- cor.test(ss_kids_df$mean_accuracy, ss_kids_df$Months)
cor_rt_age <- cor.test(ss_kids_rt_df$mean_correct_rt, ss_kids_rt_df$Months)

# get r values
acc_age_r <- as.character(round(cor_acc_age$estimate, 2))
rt_age_r <- as.character(round(cor_rt_age$estimate, 2))
# get p values
acc_age_p <- as.character(round(cor_acc_age$p.value, 3))
rt_age_p <- as.character(round(cor_rt_age$p.value, 3))
# get df 
acc_age_df <- as.character(cor_acc_age$parameter)
rt_age_df <- as.character(cor_rt_age$parameter)

# correlations acc and rt
cor_acc_rt <- cor.test(ss_kids_rt_df$mean_accuracy, ss_kids_rt_df$mean_correct_rt)
acc_rt_r <- as.character(round(cor_acc_rt$estimate, 2))  # get r values
acc_rt_p <- as.character(round(cor_acc_rt$p.value, 3))   # get p values
acc_rt_df <- as.character(cor_acc_rt$parameter)          # get df 
```

```{r t-test rt}
levene_rt <- leveneTest(mean_correct_rt ~ age_group, data=ss_kids_rt_df)
rt_ttest <- t.test(mean_correct_rt ~ age_group, alternative = "greater", 
                   var.equal=T, data = ss_kids_rt_df)

rt_tval <- round(rt_ttest$statistic, 2)
rt_tdf <- rt_ttest$parameter

## compute effect size
ms_rt <- ss_kids_rt_df %>% 
    group_by(age_group) %>% 
    summarise(mean = mean(mean_correct_rt),
              sd = sd(mean_correct_rt),
              n_group = n())

rt_cohd <- round( (ms_rt$mean[1] - ms_rt$mean[2]) / 
    sqrt( ((ms_rt$n_group[1] - 1) * ms_rt$sd[1]^2) + 
              ((ms_rt$n_group[2] - 1) * ms_rt$sd[2]^2) / 
              (ms_rt$n_group[1] + ms_rt$n_group[2] - 2) ), 2 )
```

```{r t-test acc}
levene_acc <- leveneTest(mean_accuracy ~ age_group, data=ss_kids_df)
acc_ttest <- t.test(mean_accuracy ~ age_group, alternative = "less", 
                   var.equal=T, data = ss_kids_df)

acc_tval <- round(acc_ttest$statistic, 2)
acc_tdf <- acc_ttest$parameter

## compute effect size
ms_acc <- ss_kids_df %>% 
    group_by(age_group) %>% 
    summarise(mean = mean(mean_accuracy),
              sd = sd(mean_accuracy),
              n_group = n())

acc_cohd <- round( (ms_acc$mean[2] - ms_acc$mean[1]) / 
    sqrt( ((ms_acc$n_group[1] - 1) * ms_acc$sd[1]^2) + 
              ((ms_acc$n_group[2] - 1) * ms_acc$sd[2]^2) / 
              (ms_acc$n_group[1] + ms_acc$n_group[2] - 2) ), 2 )
```

```{r correlations voc and peek}
# correlations vocab
cor_acc_voc <- cor.test(ss_kids_df$mean_accuracy, ss_kids_df$signs_produced)
cor_rt_voc <- cor.test(ss_kids_rt_df$mean_correct_rt, ss_kids_rt_df$signs_produced)
cor_age_voc <- cor.test(ss_kids_rt_df$Months, ss_kids_rt_df$signs_produced)

# get r values
acc_voc_r <- as.character(round(cor_acc_voc$estimate, 2))
rt_voc_r <- as.character(round(cor_rt_voc$estimate, 2))
age_voc_r <- as.character(round(cor_age_voc$estimate, 2))

# get p values
acc_voc_p <- as.character(round(cor_acc_voc$p.value, 3))
rt_voc_p <- as.character(round(cor_rt_voc$p.value, 3))

# get df 
acc_voc_df <- as.character(cor_acc_voc$parameter)
rt_voc_df <- as.character(cor_rt_voc$parameter)
age_voc_df <- as.character(cor_age_voc$parameter)
```

```{r, multiple regression acc}
fit_acc1 <- lm(mean_accuracy ~ signs_produced, data = ss_kids_df)
fit_acc2 <- lm(mean_accuracy ~ signs_produced + age_peek_months, data = ss_kids_df)
fit_acc3 <- lm(mean_accuracy ~ age_peek_months, data = ss_kids_df)
acc_lm_anova <- anova(fit_acc1, fit_acc2)
```

```{r, multiple regression rt}
fit_rt1 <- lm(mean_correct_rt ~ signs_produced, data = ss_kids_rt_df)
fit_rt2 <- lm(mean_correct_rt ~ signs_produced + age_peek_months, data = ss_kids_rt_df)
fit_rt3 <- lm(mean_correct_rt ~ age_peek_months, data = ss_kids_rt_df)
rt_lm_anova <- anova(fit_rt1, fit_rt2)
```

```{r profile plot png, cache=T, fig.width=6, fig.height=4, fig.cap="The timecourse of participants' responses to the target picture in relation to the unfolding sign for younger children, older children, and adults. Curves show changes over time in the mean proportion looking to the correct picture, measured in ms from noun onset; error bars represent +/- 95% CI computed by non-parametric bootstrap. The dashed vertical line indicates mean offset of target nouns (1134 ms)."}

pp_png <- png::readPNG("Figs/profile_plot.png")
grid::grid.raster(pp_png)
```

First we present an overview of performance on the VLP task, showing that children become faster and more accurate at comprehending familiar signs as they get older and make progress towards adult levels of language fluency. Then we present analyses of the links between children’s real-time ASL processing skills and both age and productive ASL vocabulary. 

### ASL processing efficiency

Figure 2 provides an overview of the timecourse of correct orienting to the referent in response to the target sign. The three curves show changes in the mean proportion of trials on which participants in each age group fixated the correct referent at every 33 ms interval as the target sign unfolded. Before seeing the target sign, all participants fixated on the signer. Interestingly, both adults and children began to increase their looking to the target picture before the offset of the target noun, providing evidence that signers were not waiting until the end of the linguistic utterance to seek out the target image. Children were slower to respond and less accurate than adults, on average, maintaining their gaze on the center signer for approximately 700 ms and reaching a lower asymptote. 

REPORT MEANS FOR ADULTS [STATS HERE?]

The youngest children took longer to orient to the target ($Myounger = `r round(ms_rt$mean[2],0)`$ ms, $Molder = `r round(ms_rt$mean[1],0)`$ ms, $t(`r rt_tdf`) = `r rt_tval`, d = `r rt_cohd`$) and were less accurate than older children ($Myounger = `r round(ms_acc$mean[1],2)`$, $Molder = `r round(ms_acc$mean[2],2)`$, $t(`r acc_tdf`) = `r acc_tval`, d = `r acc_cohd`$). 

### Links between processing efficiency and age

Mean accuracy scores, computed over the `r analysis.window[1]`–`r analysis.window[2]` ms window from noun onset, were examined as a function of age. Accuracy was strongly correlated with age ($r$(`r acc_age_df`) = `r acc_age_r`), indicating that older ASL learners were more reliable than younger children in fixating the target picture. Mean reaction times were negatively correlated with age ($r$(`r rt_age_df`) = `r rt_age_r`), indicating that older ASL-learning children were faster to shift to the target picture than younger ones. Mean reaction times were also negatively correlated with mean accuracy scores ($r$(`r acc_rt_df`) = `r acc_rt_r`) such that those children who were faster to shift to the target were also more likely to stick on the target image throughout a greater proportion of the analysis window.

Together, the Accuracy and Reaction Time analyses show that signers will reliably leave a central signer to shift to a target image in the VLP task, even before the end of the linguistic utterance. Importantly, signers varied in their response times and accuracy, and this variation was meaningfully linked to age. Thus, like children learning spoken language, ASL learners improve their real-time language processing skills over the second and third years of life, progressing towards adult levels of language fluency. 

```{r vocab scatter plots, fig.height = 5, fig.width = 10, fig.cap="Relationship between VLP measures and productive ASL vocabulary. Each data point is an individual child. Panel A shows the positive relationship between accuracy and vocabulary. Panel B shows the negative relationship between RT and vocabulary"}
acc_voc_plot <- qplot(x = signs_produced, y = mean_accuracy, data = ss_kids_df,
      ylab=c("Accuracy"), 
      xlab=c("Signs Produced")) +
    ylim(0.44, 0.85) +
    xlim(15, 85) +
    geom_smooth(method = "lm", se=F, color="black", size = 1.5) +
    geom_point(color = "black", size = 5.5) +
    geom_point(color = "grey50", size = 4) +
    annotate("text", x = 30, y = 0.8, label = paste("r =", acc_voc_r), size=8) +
    theme_bw() + 
    theme(axis.title.x = element_text(colour="grey30",size=22,
                                      angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.title.y = element_text(colour="grey30",size=22,
                                      hjust=0.5,vjust=0.5,face="plain"),
          axis.text.x = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.text.y = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          panel.grid.major=element_blank())

rt_voc_plot <- qplot(x = signs_produced, y = mean_correct_rt, data = ss_kids_rt_df,
      ylab=c("Reaction Time (ms)"), 
      xlab=c("Signs Produced")) +
    ylim(800, 1950) +
    xlim(15, 85) +
    geom_smooth(method = "lm", se=F, color="black", size = 1.5) +
    geom_point(color = "black", size = 5.5) +
    geom_point(color = "grey50", size = 4) +
    annotate("text", x = 65, y = 1750, label = paste("r =", rt_voc_r), size=8) +
    theme_bw() + 
    theme(axis.title.x = element_text(colour="grey30",size=22,
                                      angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.title.y = element_text(colour="grey30",size=22,
                                      hjust=0.5,vjust=0.5,face="plain"),
          axis.text.x = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.text.y = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          panel.grid.major=element_blank())

gridExtra::grid.arrange(acc_voc_plot, rt_voc_plot, ncol = 2)
```
 
### Links between processing efficiency and vocabulary

Figure 3 shows the relationships between both VLP processing measures and children's productive ASL vocabulary. Mean accuracy was positively related to vocabulary size ($r$(`r acc_voc_df`) = `r acc_voc_r`) such that children with higher accuracy scores also had larger productive vocabularies. Mean reaction times were negatively correlated with vocabulary ($r$(`r rt_voc_df`) = `r rt_voc_r`) indicating that children who were faster to recognize ASL signs were those children with larger sign vocabularies. 

It is important to point out that age and vocabulary were strongly intercorrelated in our sample ($r$(`r age_voc_df`) = `r age_voc_r`). Multiple regression analyses indicated that together these factors accounted for approximately `r round(summary(fit_acc2)$r.squared, 3)*100`% of the variance in accuracy ($F$(`r round(summary(fit_acc2)$fstatistic[2])`, `r round(summary(fit_acc2)$fstatistic[3])`) = `r round(summary(fit_acc2)$fstatistic[1], 1)`). Although vocabulary did not contribute significant variance after age was taken into account ($r^2$-change: `r round(summary(fit_acc2)$r.squared, 3) * 100 - round(summary(fit_acc1)$r.squared, 3) * 100`%), age contributed approximately `r round(summary(fit_acc3)$r.squared, 3) * 100 - round(summary(fit_acc1)$r.squared, 3) * 100`% additional variance beyond vocabulary. Thus, the majority of the variation in accuracy that was accounted for by age and vocabulary size was attributable to the shared variance between these two factors, yet some sources of individual differences in accuracy were attributable to age above and beyond vocabulary. 

****************

Multiple regression analyses also indicated that age and vocabulary together accounted for approximately `r round(summary(fit_rt2)$r.squared, 3) * 100`% of the variance in RT. However, in contrast to the accuracy measure, neither age nor vocabulary contributed significant unique variance ($r^2$-change: `r round(summary(fit_rt2)$r.squared, 3) * 100 - round(summary(fit_rt1)$r.squared, 3) * 100`%) on the RT measure. Thus, all of the variation in RT accounted for by age and vocabulary was attributable to the shared variance between these two factors. 

Taken together, these results indicate that children learning ASL were more accurate and efficient in identifying the referents of familiar signs as they got older and developed a larger expressive vocabulary. These findings are consistent with previous research with children learning English and Spanish [@hurtado2007spoken; @fernald2006picking]. 

# Discussion 

Establishing reference in real-time is a fundamental component of language learning. To link signs to objects, young ASL users must learn to resolve an apparent conflict between attending to the source of linguistic information and shifting their gaze to the surrounding visual scene. Moreover, they must learn to do this efficiently because language unfolds rapidly, and if a child does not see a sign, or does not see the intended referent, the information in that naming event is effectively unavailable. With this study, we aimed to develop and validate the first measures of young ASL learners’ real-time language comprehension skills and explore the links between these skills and both age and vocabulary. There are three main findings from this work. 

First, measuring gaze shifts during real-time sentence processing is a valid way to measure young ASL learners' language development. Previous work using online measures with adult ASL users has revealed important aspects of the psycholinguistics of sentence processing. For example, @lieberman2014real showed evidence of real-time activation of sublexical features of sign and that incremental semantic processing occurs during real-time sign comprehension. But we did not yet know whether these online measures would work with very young learners. Moreover, because gaze serves a variety of functions in ASL, using eye movements as an index of language comprehension ability might not have been possible. For example, proficient ASL users rely on vision for: (a) processing linguistic information, (b) processing the visual world, (c) regulating turn-taking during conversation [@baker1978focusing], and (d) role shifts during narrative production [@bahan1995line]. However, despite these modality-driven differences, the current study showed that both adults and children reliably increase their tendecy to look toward to the target picture before the end of the target noun, showing the signature incremental processing skills thought to be so important for fluent language comprehension. It is also interesting to consider how the rapid gaze shifts we measured in the VLP paradigm would compare to the gaze shifting behavior required in naturalistic interactions to establish mutual gaze to objects (i.e., joint attention), which is thought to be critical for early lexical development (see @lieberman2014learning for a discussion of joint attention behavior in ASL). 

The second main finding was that, like children learning spoken language [@fernald1998rapid], young ASL learners' show measurable age-related improvement in the efficiency with which they processed language. All of the target signs were familiar to children in this age range, yet older children more quickly and accurately identified the correct referents than younger children. This finding provides additional evidence that ASL acquisition in native, natural contexts follows a strikingly similar developmental path to children learning spoken language [@lillo1999modality; @mayberry2006sign]. Moreover, this is the first study to show that real-time ASL *comprehension* skills are linked to early language development. Prior work on the developmental trajectories of deaf children has relied on aspects of language production often because production is easier to see, making it easier to measure. However, it is a well-known phenomenon in language acquisition  that comprehension tends to precede production (see @clark2009first). Thus, by developing a fine-grained measure of real-time ASL comprehension, we have provided a tool which can measure children's language skills earlier in development than previously possible.

The third result was the discovery of a link between early ASL processing skills and children's productive ASL vocabularies. We found that ASL-learning children who knew more signs were also faster and more accurate in language processing than those who were lexically less advanced. However, the factors of age and vocabulary size were highly intercorrelated in this sample and the majority of the associations between vocabulary and efficiency of language processing were attributable to variance that was shared between these two factors. Nevertheless, these results with children learning ASL are consistent with other studies with English- and Spanish- learning children, which find strong relations between efficiency in online language comprehension and other concurrent and longitudinal measures of linguistic achievement [@fernald2001half; @fernald2006picking; @zangl2005dynamics].  ADD REF Marchman & Fernald (2008).
It is important to point out that the direction of the relationship between vocabulary and processing skills is unclear (for a detailed discussion see @hurtado2007spoken). It could be that initial differences in processing speed makes it easier for some children to learn words more quickly. It is also likely that having a larger vocabulary facilitates sign processing. It might also be the case that knowing more signs is associated with more efficient word-recognition skills because lexical growth has led to changes in the way that lexical forms are represented. Thus, children with larger vocabularies may be faster and more efficient processors of spoken language because lexical growth itself has contributed to a shift to more segmentally-based lexical representations. Like our studies with spoken languages, our ongoing research will continue to examine the nature and direction of these links.

This study is not without limitations.  While our study is way larger than most, it is still a small sample. We could put up on github - open science plug here.

Another feature of our sample was the broad age range.  Recall that age sopped up a lot.  If we tested a tighter age range, we might have seen a more independent effect of vocabulary size, as other studies with that design do. 

While some of the kids are deaf and some are hearing, this is a special case of children who are all native signers with exposure from birth. We might anticipate that the development of online language processing might look different in children who are late learners of sign. and/or who have much more heterogeneous and inconsistent language exposure than the ones tested in the current work.  An important important next step is to explore the relations between the development of language processing skills explored here and children's experience with signed languages during interactions with caregivers.  It is well-known that children's efficiency of online processing is tightly linked to the quantity and quality of the speech that they hear.  We woudl expect similar relations in children learning ASL.


In sum, this study provides the first evidence that eye movements during real-time ASL sentence processing provide a valid measure of age-related changes in young children's visual language comprehension skills. These findings contribute to the now significant body of literature highlighting the parallels between signed and spoken language development when children are exposed to native sign input. We hope that the development of the VLP task will provide a useful method for researchers and educators, providing a way to track developmental trajectories of early language learning in native learners of signed languages, like ASL.  

# Acknowledgements

We are grateful to the many children and parents who participated in this research, and to the staff of the California School for the Deaf in Fremont. Special thanks to Shane Blau, Kat Adams, Melanie Ashland, and the staff of the Language Learning Lab at Stanford University. This work was made possible by an NIDCD grant to Anne Fernald and David Corina (R21 DC012505-1).

\newpage 

# References 
