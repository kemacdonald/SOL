---
title: "SOL-bayesian-data-analysis"
author: "Kyle MacDonald"
date: "June 19, 2015"
output: html_document
---

## From Lee and Wagenmakers Chapter 6: Latent Mixture Models

Suppose a group of 15 people sit an exam made up of 40 true-or-false questions, and they get 21, 17, 21, 18, 22, 31, 31, 34, 34, 35, 35, 36, 39, 36, and 35 right. These scores suggest that the first 5 people were just guessing, but the last 10 had some level of knowledge.

One way to make statistical inferences along these lines is to assume there are two different groups of people. These groups have different probabilities of success, with the guessing group having a probability of 0.5, and the knowledge group having a probability greater than 0.5. Whether each person belongs to the first or the second group is a latent or unobserved variable that can take just two values. Using this approach, the goal is to infer to which group each person belongs, and also the rate of success for the knowledge group.

This type of model is known as a latent-mixture model, because the data are assumed to be generated by two different processes that combine or mix, and im- portant properties of that mixture are unobserved or latent. In this case, the two components that mix are the guessing and knowledge processes, and the group membership of each person is latent.

```{r}
rm(list=ls())
library(knitr)
library(R2jags)
library(gridExtra)
library(reshape2)
library(ggplot2)
library(dplyr)
library(MASS)

df_correct <- read.csv("sol_ss_kids.csv")
df_correct <- mutate(df_correct, total_trials_shifting = C_T_count + C_D_count)
df_filt <- filter(df_correct, exclude_chance_shifter == "include")
```

First I start with multiple regression

```{r}
fit1_rt <- lm(mean_rt ~ signs_produced, data=df_correct)
fit2_rt <- lm(mean_rt ~ Months + signs_produced, data=df_correct)

anova(fit1, fit2)

fit1_acc <- lm(mean_accuracy ~ signs_produced, data=df_correct)
fit2_acc <- lm(mean_accuracy ~ Months + signs_produced, data=df_correct)

anova(fit1_acc, fit2_acc)
```

```{r}
fit1_rt <- lm(mean_rt ~ signs_produced, data=df_filt)
fit2_rt <- lm(mean_rt ~ Months + signs_produced, data=df_filt)

summary(fit1_rt)
summary(fit2_rt)

anova(fit1, fit2)

fit1_acc <- lm(mean_accuracy ~ signs_produced, data=df_filt)
fit2_acc <- lm(mean_accuracy ~ Months + signs_produced, data=df_filt)

summary(fit1_acc)
summary(fit2_acc)

anova(fit1_acc, fit2_acc)
```

Then we try robust regression 

```{r}
fit1_rlm <- rlm(mean_rt ~ signs_produced, data=df_correct)
fit2_rlm <- rlm(mean_rt ~ signs_produced + Months, data=df_correct)
anova(fit1_rlm, fit2_rlm)

summary(rlm(mean_accuracy ~ signs_produced + Months, data=df_correct))
```

Here I adjusted the model to allow for different number of trials for each participants.
To do this, I changed n to n[i] in the for loop.

```{r 6.1.1,fig.width=12, fig.height=6}
cat('# Exam Scores
model{
  # Each Person Belongs To One Of Two Latent Groups
  for (i in 1:p){
    z[i] ~ dbern(0.5)
  }
  # First Group Guesses
  psi <- 0.5
  # Second Group Has Some Unknown Greater Rate Of Success
  phi ~ dbeta(1,1)I(0.5,1)
  # Data Follow Binomial With Rate Given By Each Persons Group Assignment
  for (i in 1:p){
    theta[i] <- equals(z[i],0)*psi+equals(z[i],1)*phi
    k[i] ~ dbin(theta[i],n[i])
  }
}', file={f<-tempfile()})

k <- df_correct$C_T_count # number correct
p <- length(k) # number of people
n <- df_correct$total_trials_shifting

data <- list("p", "k", "n") # to be passed on to JAGS)
myinits <-  list(
    list(phi = 0.75, z = round(runif(p)))) # Initial group assignment

# parameters to be monitored:	
parameters <- c("phi","z")

samples <- jags(data, inits=myinits, parameters,
                model.file =f, n.chains=1, n.iter=1000, 
                n.burnin=1, n.thin=1, DIC=T)

df <- data.frame(phi = samples$BUGSoutput$sims.list$phi,
                 z = samples$BUGSoutput$sims.list$z)

# grab subject numbers
colnames(df)[which(names(df)=="z.1"):which(names(df)=="z.29")] <- as.character(df_correct$Sub.Num)

# melt data frame
df_melt <- melt(df[,2:length(df)], variable.name = "Sub.Num", value.name = "group_membership")

a <- qplot(data=melt(df$phi),x=value,geom='histogram',binwidth=0.008)+
    theme_bw()+
    xlab('phi')+
    xlim(0,1)

b <- ggplot(data=df_melt,aes(x=factor(group_membership),fill=factor(group_membership)))+
    facet_wrap(~Sub.Num,scales='fixed')+
    geom_histogram(binwidth=0.1)+
    guides(fill=F) +
    theme_bw()+
    xlab('z (group membership): 0=Guessing, 1=Not-guessing')

grid.arrange(a,b,nrow=1, main='Group success rate and individual group membership')
```

Get the participants flagged as likely guessers. So we can compare across model specifications.

```{r}
m1_guessers <- df_melt %>% 
    group_by(Sub.Num) %>% 
    summarise(count = sum(group_membership)) %>% 
    filter(count <= 500) %>% 
    print()
```

## Compare inference from latent mixture model to binomial test across participants

```{r}
binom_guessers <- df_correct %>% 
    group_by(Sub.Num) %>% 
    summarise(binom_estimate = binom.test(C_T_count, total_trials_shifting)$estimate,
              binom_p = binom.test(C_T_count, total_trials_shifting)$p.value) %>% 
    filter(binom_p >= .10)
```

### Change prior expectation of guessing

What happens if you change the initial expectation that everybody is equally likely to belong to either group, and have an expectation that people generally are not guessing, with (say), z_i ~ Bernoulli(0.9)?


```{r 6.1.5,fig.width=12, fig.height=6, echo=FALSE}
cat('# Exam Scores
model{
  # Each Person Belongs To One Of Two Latent Groups
  for (i in 1:p){
    z[i] ~ dbern(0.9)
  }
  # First Group Guesses
  psi <- 0.5
  # Second Group Has Some Unknown Greater Rate Of Success
  phi ~ dunif(0,1)
  # Data Follow Binomial With Rate Given By Each Persons Group Assignment
  for (i in 1:p){
    theta[i] <- equals(z[i],0)*psi+equals(z[i],1)*phi
    k[i] ~ dbin(theta[i],n[i])
  }
}', file={f<-tempfile()})


data <- list("p", "k", "n") # to be passed on to JAGS
myinits <-  list(
    list(phi = 0.75, z = round(runif(p)))) # Initial group assignment

# parameters to be monitored:  
parameters <- c("phi","z")

samples <- jags(data, inits=myinits, parameters,
                model.file =f, n.chains=1, n.iter=1000, 
                n.burnin=1, n.thin=1, DIC=T)

df<-data.frame(phi = samples$BUGSoutput$sims.list$phi,
               z = samples$BUGSoutput$sims.list$z)

# grab subject numbers
colnames(df)[which(names(df)=="z.1"):which(names(df)=="z.29")] <- as.character(df_correct$Sub.Num)

df_melt <- melt(df[,2:length(df)], variable.name = "Sub.Num", value.name = "group_membership")


a <- qplot(data=melt(df$phi),x=value,geom='histogram',binwidth=0.008)+
    theme_bw()+
    xlab('phi')+
    xlim(0,1)

b <- ggplot(data=df_melt,aes(x=factor(group_membership),fill=factor(group_membership)))+
    facet_wrap(~Sub.Num,scales='fixed')+
    geom_histogram(binwidth=0.1)+
    guides(fill=F) +
    theme_bw()+
    xlab('z (group membership): 0=Guessing, 1=Not-guessing')

grid.arrange(a,b,nrow=1, main='Success rate and group+5 membership, Uniform(0,1) Prior \n z_i~Bernoulli(0.9)')
```

Get the participants flagged as likely guessers. So we can compare across model specifications.

```{r}
m2_guessers <- df_melt %>% 
    group_by(Sub.Num) %>% 
    summarise(count = sum(group_membership)) %>% 
    filter(count <= 500) %>% 
    print()
```


# Add individual differences 

The previous example shows how sampling can model data as coming from a mixture of sources, and infer properties of these latent groups. But the specific model has at least one big weakness, which is that it assumes all the people in the knowledge group have exactly the same rate of success on the questions.

One straightforward way to allow for individual differences in the knowledge group is to extend the model hierarchically. One convenient (but not perfect) choice for this “individual differences” distribution is a Gaussian. It has the problem of allowing for success rates below zero and above one. An inelegant but practical and effective way to deal with this is simply to restrict the sampled success rates to the valid range.


```{r 6.2.1 ,fig.width=12, fig.height=6}
cat('# Exam Scores With Individual Differences
model{
  # Data Follow Binomial With Rate Given By Each Persons Group Assignment
  for (i in 1:p){
    theta[i] <- equals(z[i],0)*psi+equals(z[i],1)*phi[i]
    k[i] ~ dbin(theta[i],n[i])
  }
  # Each Person Belongs To One Of Two Latent Groups
  for (i in 1:p){
    z[i] ~ dbern(0.5)
  }
  # The Second Group Allows Individual Differences
  for (i in 1:p){
    # Second Group Drawn From A Censored Gaussian Distribution
    phi[i] ~ dnorm(mu,lambda)T(0,1)
  }   
  # First Group Guesses
  psi <- 0.5
  # Second Group Mean, Precision (And Standard Deviation)
  mu ~ dbeta(1,1)T(.5,1) # >0.5 Average Success Rate
  lambda ~ dgamma(.001,.001)
  sigma <- 1/sqrt(lambda) 
  # Posterior Predictive For Second Group
  predphi ~ dnorm(mu,lambda)T(0,1)
}', file={f<-tempfile()})


data <- list("p", "k", "n") # to be passed on to JAGS
myinits <-  list(
    list(mu = 0.75, lambda = 1, z = round(runif(p)))) # Initial group assignment

# parameters to be monitored:	
parameters <- c("predphi","theta","z","mu","sigma")

# The following command calls JAGS with specific options.
# For a detailed description see the R2jags documentation.
samples <- jags(data, inits=myinits, parameters,
                model.file =f, n.chains=1, n.iter=2000, 
                n.burnin=1000, n.thin=1, DIC=T)


df<-data.frame(predphi = samples$BUGSoutput$sims.list$predphi,
               theta = samples$BUGSoutput$sims.list$theta,
               z = samples$BUGSoutput$sims.list$z,
               mu = samples$BUGSoutput$sims.list$mu,
               sigma = samples$BUGSoutput$sims.list$sigma)

# grab subject numbers
colnames(df)[which(names(df)=="z.1"):which(names(df)=="z.29")] <- as.character(df_correct$Sub.Num)

df_melt <- melt(df[,which(names(df)=="30009"):which(names(df)=="30088")], variable.name = "Sub.Num", value.name = "group_membership")


a<-qplot(data=melt(df$mu),x=value,geom='histogram',binwidth=0.008)+
    theme_bw()+
    xlab('mu')+
    xlim(0,1)

b <- ggplot(data=df_melt,aes(x=factor(group_membership),fill=factor(group_membership)))+
    facet_wrap(~Sub.Num,scales='fixed')+
    geom_histogram(binwidth=0.1)+
    guides(fill=F) +
    theme_bw()+
    xlab('z (group membership): 0=Guessing, 1=Not-guessing')

grid.arrange(a,b,nrow=1, main='Success rate and group membership w/ individual differences \n z_i~Bernoulli(0.5): Prior expectation that kids are not guessing')
```

Get the participants flagged as likely guessers. So we can compare across model specifications.

```{r}
m3_guessers <- df_melt %>% 
    group_by(Sub.Num) %>% 
    summarise(count = sum(group_membership)) %>% 
    filter(count <= 500) %>% 
    print()
```


## Compare inferences about guessing across models

```{r}
kable(m1_guessers, caption = "Uninformative Prior")
kable(m2_guessers, caption = "Strong prior expectation that participants are not guessing")
kable(m3_guessers, caption = "Uninformative prior with individual differences in knowledge")
kable(binom_guessers, caption = "Binomial test for individual participants")
```


####  Pearson correlation

Rather than just having a single number to measure the correlation, it would be nice to have a posterior distribution for r, saying how likely each possible level of correlation was. There are frequentist confidence interval methods that try to do this, as well as various analytic Bayesian results based on asymptotic approx- imations (e.g., Donner & Wells, 1986). An advantage of using a computational approach is the flexibility in the assumptions that can be made. It is possible to set up a graphical model that allows inferences about the correlation coefficient for any set of prior assumptions about the correlation.

```{r 5.1}
 x <- matrix(c(.8,102, 1,98, .5,100, 0.9,105, .7,103, 
               0.4,110, 1.2,99, 1.4,87, 0.6,113, 1.1,89, 1.3,93),
             nrow=11,ncol=2,byrow=T) 

n <- nrow(x) # number of people/units measured

data <- list("x", "n") # to be passed on to JAGS
myinits <- list(
    list(r = 0, mu = c(0,0), lambda = c(1,1)))
# parameters to be monitored:  
parameters <- c("r", "mu", "sigma")

cat('# Pearson Correlation
model{
  # Data
  for (i in 1:n){
    x[i,1:2] ~ dmnorm(mu[],TI[,])
  }
  # Priors
  mu[1] ~ dnorm(0,.001)
  mu[2] ~ dnorm(0,.001)
  lambda[1] ~ dgamma(.001,.001)
  lambda[2] ~ dgamma(.001,.001)
  r ~ dunif(-1,1)
  # Reparameterization
  sigma[1] <- 1/sqrt(lambda[1])
  sigma[2] <- 1/sqrt(lambda[2])
  T[1,1] <- 1/lambda[1]
  T[1,2] <- r*sigma[1]*sigma[2]
  T[2,1] <- r*sigma[1]*sigma[2]
  T[2,2] <- 1/lambda[2]
  TI[1:2,1:2] <- inverse(T[1:2,1:2])
}', file={f<-tempfile()})

# The following command calls JAGS with specific options.
# For a detailed description see the R2jags documentation.
samples = jags(data, inits=myinits, parameters,
               model.file =f, n.chains=1, n.iter=5000, 
               n.burnin=1, n.thin=1, DIC=T)

# Now the values for the monitored parameters are in the "samples" object, 
# ready for inspection.

dr <- data.frame(r=samples$BUGSoutput$sims.list$r)
dx <- data.frame(x)
#Frequentist point-estimate of r:
freq.r <- with(dx,cor(X1,X2))

a<-qplot(data=dx,x=X1,y=X2,geom='point')+theme_bw()+
    geom_smooth(method = "lm")+
    xlab('response time (sec)')+
    ylab('IQ')

b<-ggplot(data=dr,aes(x=r))+
    geom_density()+
    geom_vline(xintercept = freq.r, linetype='longdash')+
    theme_bw()+
    xlab('correlation')+
    xlim(-1,0.5)

#quartz('correlation, dataset 1')
grid.arrange(a,b,nrow=1)
```


Exercise 5.1.1 The second data set in the Matlab and R code is just the first data set from Figure 5.2 repeated twice. Set dataset=2 to consider these repeated data, and interpret the differences in the posterior distributions for r.

```{r 5.1.1}
x <- matrix(c(.8,102, 1,98, .5,100, 0.9,105, .7,103, 
              0.4,110, 1.2,99, 1.4,87, 0.6,113, 1.1,89, 1.3,93,
              .8,102, 1,98, .5,100, 0.9,105, .7,103, 
              0.4,110, 1.2,99, 1.4,87, 0.6,113, 1.1,89, 1.3,93),
            nrow=22,ncol=2,byrow=T) 

n <- nrow(x) # number of people/units measured

data <- list("x", "n") # to be passed on to JAGS
myinits <- list(
    list(r = 0, mu = c(0,0), lambda = c(1,1)))
# parameters to be monitored:  
parameters <- c("r", "mu", "sigma")

# The following command calls JAGS with specific options.
# For a detailed description see the R2jags documentation.
samples = jags(data, inits=myinits, parameters,
               model.file =f, n.chains=1, n.iter=5000, 
               n.burnin=1, n.thin=1, DIC=T)
# Now the values for the monitored parameters are in the "samples" object, 
# ready for inspection.

dr <- data.frame(r=samples$BUGSoutput$sims.list$r)
dx <- data.frame(x)
#Frequentist point-estimate of r:
freq.r <- with(dx,cor(X1,X2))

c<-qplot(data=dx,x=X1,y=X2,geom='point')+theme_bw()+
    geom_smooth(method="lm")+
    xlab('response time (sec)')+
    ylab('IQ')
d<-ggplot(data=dr,aes(x=r))+
    geom_density()+
    geom_vline(xintercept = freq.r, linetype='longdash')+
    theme_bw()+
    xlab('correlation')+
    xlim(-1,0.5)

#quartz('correlation, dataset 2')
grid.arrange(c,d,nrow=1)
```

Exercise 5.1.2 Do you find the priors on μ1 and μ2 to be reasonable?

The priors on the means are gaussians with low precision. The mean for each is 0, however the means for these two measures are quite different (IQ _ mean ~ 100; RT _ mean ~ 1). Perhaps this is worth changing. Also, we could import our knowledge of the IQ test into the prior for IQ scores.

Let's try that.

```{r 5.1.2}
x <- matrix(c(.8,102, 1,98, .5,100, 0.9,105, .7,103, 
              0.4,110, 1.2,99, 1.4,87, 0.6,113, 1.1,89, 1.3,93),
            nrow=11,ncol=2,byrow=T) 

n <- nrow(x) # number of people/units measured

data <- list("x", "n") # to be passed on to JAGS
myinits <- list(
    list(r = 0, mu = c(0,0), lambda = c(1,1)))
# parameters to be monitored:  
parameters <- c("r", "mu", "sigma")

cat('# Pearson Correlation
model{
  # Data
  for (i in 1:n){
    x[i,1:2] ~ dmnorm(mu[],TI[,])
  }
  # Priors
  mu[1] ~ dnorm(1,.001)
  mu[2] ~ dnorm(100,.0044)
  lambda[1] ~ dgamma(.001,.001)
  lambda[2] ~ dgamma(.001,.001)
  r ~ dunif(-1,1)
  # Reparameterization
  sigma[1] <- 1/sqrt(lambda[1])
  sigma[2] <- 1/sqrt(lambda[2])
  T[1,1] <- 1/lambda[1]
  T[1,2] <- r*sigma[1]*sigma[2]
  T[2,1] <- r*sigma[1]*sigma[2]
  T[2,2] <- 1/lambda[2]
  TI[1:2,1:2] <- inverse(T[1:2,1:2])
}', file = {g <- tempfile()})

# The following command calls JAGS with specific options.
# For a detailed description see the R2jags documentation.
samples = jags(data, inits=myinits, parameters,
               model.file =g, n.chains=1, n.iter=5000, 
               n.burnin=1, n.thin=1, DIC=T)
# Now the values for the monitored parameters are in the "samples" object, 
# ready for inspection.

dr <- data.frame(r=samples$BUGSoutput$sims.list$r)
dx <- data.frame(x)
#Frequentist point-estimate of r:
freq.r <- with(dx,cor(X1,X2))

a<-qplot(data=dx,x=X1,y=X2,geom='point')+theme_bw()+
    geom_smooth(method="lm") +
    xlab('response time (sec)')+
    ylab('IQ')
b<-ggplot(data=dr,aes(x=r))+
    geom_density()+
    geom_vline(xintercept = freq.r, linetype='longdash')+
    theme_bw()+
    xlab('correlation')+
    xlim(-1,0.5)

#quartz('correlation, dataset 1, alt priors')
grid.arrange(a,b,nrow=1)
```

Exercise 5.1.3 The current graphical model assumes that the values from the two variables—the xi = (xi1, xi2)—are observed with perfect accuracy. When might this be a problematic assumption? How could the current approach be extended to make more realistic assumptions?

Ans: measurement error in tests... IQ tests have some noise, RT data has some noise. But is this already captured by sigma?

### 5.2 Person correlation with uncertainty

The observed data still take the form xi = (xi1,xi2) for the ith person’s response time and IQ measure. But these observations are now sampled from a Gaussian distribution, centered on the unobserved true response time and IQ of that person, denoted yi = (yi1, yi2). These true values are then modeled as the x were in the previous model in Figure 5.1, as draws from a multivariate Gaussian distribution.


```{r 5.2.1}
x <- matrix(c(.8,102, 1,98, .5,100, 0.9,105, .7,103, 
              0.4,110, 1.2,99, 1.4,87, 0.6,113, 1.1,89, 1.3,93),
            nrow=11,ncol=2,byrow=T) 
n <- nrow(x) # number of people/units measured

#precision of measurement:
sigmaerror <- c(.03, 1) # both measurements quite precise
# sigmaerror = c(.03, 10)
lambdaerror <- 1/sigmaerror^2

data <- list("x", "n", "lambdaerror") # to be passed on to JAGS
myinits <-  list(
    list(r = 0, mu = c(0,0), lambda = c(1,1)))
# parameters to be monitored:	
parameters <- c("r", "mu", "sigma")

cat('# Pearson Correlation With Uncertainty in Measurement
model{
  # Data
  for (i in 1:n){
    y[i,1:2] ~ dmnorm(mu[],TI[,])
    for (j in 1:2){
      x[i,j] ~ dnorm(y[i,j],lambdaerror[j])
    }
  }
  # Priors
  mu[1] ~ dnorm(0,.001)
  mu[2] ~ dnorm(0,.001)
  lambda[1] ~ dgamma(.001,.001)
  lambda[2] ~ dgamma(.001,.001)
  r ~ dunif(-1,1)
  # Reparameterization
  sigma[1] <- 1/sqrt(lambda[1])
  sigma[2] <- 1/sqrt(lambda[2])
  T[1,1] <- 1/lambda[1]
  T[1,2] <- r*sigma[1]*sigma[2]
  T[2,1] <- r*sigma[1]*sigma[2]
  T[2,2] <- 1/lambda[2]
  TI[1:2,1:2] <- inverse(T[1:2,1:2])
}', file= {correlation_2<- tempfile()})


# The following command calls JAGS with specific options.
# For a detailed description see the R2jags documentation.
samples <- jags(data, inits=myinits, parameters,
                model.file = correlation_2, n.chains=1, n.iter=5000, 
                n.burnin=1, n.thin=1, DIC=T)
# Now the values for the monitored parameters are in the "samples" object, 
# ready for inspection.

dr <- data.frame(r=samples$BUGSoutput$sims.list$r)

dx <- data.frame(x)
#Frequentist point-estimate of r:
freq.r <- with(dx,cor(X1,X2))
limits <- aes(xmin=, xmax =,
              ymax = resp + se, ymin=resp - se)

a<-ggplot(data=dx,aes(x=X1,y=X2))+
    geom_point()+
    geom_errorbarh(aes(xmin=X1-sigmaerror[1],xmax=X1+sigmaerror[1]),height=0.001)+
    geom_errorbar(aes(ymin=X2-sigmaerror[2],ymax=X2+sigmaerror[2]),width=0.001)+
    theme_bw()+
    xlab('response time (sec)')+
    ylab('IQ')


b<-ggplot(data=dr,aes(x=r))+
    geom_density()+
    geom_vline(xintercept = freq.r, linetype='longdash')+
    theme_bw()+
    xlab('correlation')+
    xlim(-1,0.5)

#quartz('correlation with uncertainty, dataset 1')
grid.arrange(a,b,nrow=1)
```

Exercise 5.2.2 Generate results for the second data set, which changes σ = 10 for the IQ measurement. Compare these results with those obtained assuming σ = 1 .

```{r 5.2.2}
x <- matrix(c(.8,102, 1,98, .5,100, 0.9,105, .7,103, 
              0.4,110, 1.2,99, 1.4,87, 0.6,113, 1.1,89, 1.3,93),
            nrow=11,ncol=2,byrow=T) 
n <- nrow(x) # number of people/units measured

#precision of measurement:
#sigmaerror <- c(.03, 1) # both measurements quite precise
sigmaerror = c(.03, 10) # IQ measure is less precise with this parameterization
lambdaerror <- 1/sigmaerror^2

data <- list("x", "n", "lambdaerror") # to be passed on to JAGS
myinits <-  list(
    list(r = 0, mu = c(0,0), lambda = c(1,1)))
# parameters to be monitored:  
parameters <- c("r", "mu", "sigma")

# The following command calls JAGS with specific options.
# For a detailed description see the R2jags documentation.
samples <- jags(data, inits=myinits, parameters,
                model.file = correlation_2, n.chains=1, n.iter=5000, 
                n.burnin=1, n.thin=1, DIC=T)
# Now the values for the monitored parameters are in the "samples" object, 
# ready for inspection.

dr <- data.frame(r=samples$BUGSoutput$sims.list$r)

dx <- data.frame(x)
#Frequentist point-estimate of r:
freq.r <- with(dx,cor(X1,X2))
limits <- aes(xmin=, xmax =,
              ymax = resp + se, ymin=resp - se)

a<-ggplot(data=dx,aes(x=X1,y=X2))+
    geom_point()+
    geom_errorbarh(aes(xmin=X1-sigmaerror[1],xmax=X1+sigmaerror[1]),height=0.001)+
    geom_errorbar(aes(ymin=X2-sigmaerror[2],ymax=X2+sigmaerror[2]),width=0.001)+
    theme_bw()+
    xlab('response time (sec)')+
    ylab('IQ')


b<-ggplot(data=dr,aes(x=r))+
    geom_density()+
    geom_vline(xintercept = freq.r, linetype='longdash')+
    theme_bw()+
    xlab('correlation')+
    xlim(-1,0.5)

#quartz('correlation with more uncertainty on IQ, dataset 1')
grid.arrange(a,b,nrow=1)
```

Exercise 5.2.3 The graphical model in Figure 5.3 assumes the uncertainty for each variable is known. How could this assumption be relaxed to the case where the uncertainty is unknown?

PRIORS

Exercise 5.2.4 The graphical model in Figure 5.3 assumes the uncertainty for each variable is the same for all observations. How could this assumption be relaxed to the case where, for example, extreme IQs are less accurately measured than IQs in the middle of the standard distribution?

PRIORS on a subject level?

