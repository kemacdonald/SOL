---
title: "SOL Bayesian Data Analysis"
author: "Kyle MacDonald"
date: "April 4, 2016"
output: html_document
bibliography: bda.bib
---

```{r, echo = F}
rm(list=ls()) # clear workspace
knitr::opts_chunk$set(
    fig.height=4, 
    fig.width=7)

set.seed(seed = 10) # so simulations are the same each time

library(langcog)
library(knitr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(polspline)
library(BayesFactor)
library(rethinking)
library(R2jags)
library(rjags)
library(binom)
library(lme4)
library(bootstrap)
library(magrittr)
library(stringr)
theme_set(theme_bw())

find_mode <- function(x) {
    ux <- unique(x)
    ux[which.max(tabulate(match(x, ux)))]
}

HDIofMCMC = function( sampleVec , credMass=0.95 ) {
    # Computes highest density interval from a sample of representative values,
    #   estimated as shortest credible interval.
    # Arguments:
    #   sampleVec
    #     is a vector of representative values from a probability distribution.
    #   credMass
    #     is a scalar between 0 and 1, indicating the mass within the credible
    #     interval that is to be estimated.
    # Value:
    #   HDIlim is a vector containing the limits of the HDI
    sortedPts = sort( sampleVec )
    ciIdxInc = ceiling( credMass * length( sortedPts ) )
    nCIs = length( sortedPts ) - ciIdxInc
    ciWidth = rep( 0 , nCIs )
    for ( i in 1:nCIs ) {
        ciWidth[ i ] = sortedPts[ i + ciIdxInc ] - sortedPts[ i ]
    }
    HDImin = sortedPts[ which.min( ciWidth ) ]
    HDImax = sortedPts[ which.min( ciWidth ) + ciIdxInc ]
    HDIlim = c( HDImin , HDImax )
    return( HDIlim )
}
```

In this document, you will find the details of our Bayesian analysis for the paper, "Real-time Language Comprehension in American Sign Language."

```{r load data}
# Load data
d <- read.csv("../../analysis/eye_movements/sol_ss_all.csv")

# Standardize the data
d %<>% 
    mutate(C_D_count = ifelse(is.na(C_D_count), 0, C_D_count),
           total_trials_shifting = C_T_count + C_D_count,
           Sub.Num = as.character(Sub.Num)) %>% 
    dplyr::select(Sub.Num, age_peek_months, age_group, signs_produced, C_T_count, total_trials_shifting,
                  median_rt = median_ct_rt, C_T_prop, mean_prop_looking_TD,
                  value_cat, age_group_collapsed, hearing_status_participant) %>% 
    mutate(age.s = (age_peek_months - mean(age_peek_months)) / sd(age_peek_months),
           acc.s = (mean_prop_looking_TD - mean(mean_prop_looking_TD)) / sd(mean_prop_looking_TD),
           rt.s = (median_rt - mean(median_rt)) / sd(median_rt))

d_all <- d %>% filter(value_cat == "Target" | value_cat == "Distractor")
    
d %<>% filter(age_group_collapsed == "Kids", value_cat == "Target")  

d_voc <- d %>% 
    filter(is.na(signs_produced) == F) %>% 
    mutate(voc.s = (signs_produced - mean(signs_produced)) / sd(signs_produced))
```

## Abstract

The ability to interpret language rapidly is critical for developing language proficiency. Research on real-time sentence processing by very young children has used eye movements as a window into their emerging comprehension abilities (Fernald & Marchman, 2012). In this study, we developed the first measures of children’s real-time comprehension of a visual language, American Sign Language (ASL). Participants were 29 native ASL-learning children (16-53 mos, 16 deaf and 13 hearing) and fluent adult signers (n=19). Children’s ASL comprehension improved with age, moving toward the efficiency of adult signers. Importantly, variation in children’s processing efficiency was associated with vocabulary size, linking the ability to establish reference in real time with language learning. Finally, both deaf and hearing ASL learners showed qualitatively similar patterns of looking behavior, suggesting that visual language processing is driven by experience with a visual language, and not by deafness. These findings show important parallels between children learning signed and spoken languages in the early development of real-time language comprehension.

## Defining priors

RT estimate: is 33ms change per month in spoken language
Acc estimate is .0167 change per month in spoken language

## Age as categorical variable

All models fit using STAN. First we need to create dummy variables for the relevant group comparisons.

```{r dummy vars}
d_model <- d_all %>% 
    mutate(age_group_older = ifelse(age_group == ">= 27 Months", 1, 0),
           age_group_adults = ifelse(age_group == "Adults", 1, 0),
           age_group_clean = ifelse(age_group_older == 1, "Older", 
                                    ifelse(age_group_adults == 1, "Adult", 
                                           "Younger")),
           age_group_adults_collapsed = ifelse(age_group == "Adults", 1, 0),
           hearing_group = ifelse(hearing_status_participant == "hearing", 1, 0))
```

### CODA vs. Deaf

Compare accuracy for deaf and hearing kids. All priors vague since here we are just concerned with parameter estimation. 

```{r hearing_status acc}
acc_hearing_stat_cat_targ.m <- map2stan(
    alist(
        mean_prop_looking_TD ~ dnorm(mu, sigma),
        mu <- Intercept + b_Hearing*hearing_group,
        Intercept ~ dnorm(0.6, 10), 
        b_Hearing ~ dnorm(0, 1),
        sigma ~ dunif(0, 10)
    ),
    data = select(filter(d_model, value_cat == "Target", age_group_collapsed == "Kids"),
                  mean_prop_looking_TD, hearing_group))

# get samples
post_hearing_stat.acc <- extract.samples(acc_hearing_stat_cat_targ.m)
mu.hearing <- post_hearing_stat.acc$Intercept
mu.deaf <- post_hearing_stat.acc$b_Hearing + post_hearing_stat.acc$Intercept
post_df_hearing_stat.acc <- data.frame(mu.hearing, mu.deaf)
precis(post_df_hearing_stat.acc)

# test contrast
diff.deaf.hearing <- mu.hearing - mu.deaf
quantile(diff.deaf.hearing, probs = c(0.025, 0.5, 0.975))
```

```{r hearing_status rt}
rt_hearing_stat.m <- map2stan(
    alist(
        median_rt ~ dnorm(mu, sigma),
        mu <- Intercept + b_Hearing*hearing_group,
        Intercept ~ dnorm(1300, 100), 
        b_Hearing ~ dnorm(0, 500),
        sigma ~ dunif(0, 500)
    ),
    data = select(filter(d_model, value_cat == "Target", age_group_collapsed == "Kids"), 
                  median_rt, hearing_group)
)

# extract samples
post_hearing_stat.rt <- extract.samples(rt_hearing_stat.m)
mu.hearing.rt <- as.numeric(post_hearing_stat.rt$Intercept)
mu.deaf.rt <- as.numeric(post_hearing_stat.rt$b_Hearing + post_hearing_stat.rt$Intercept)
post_hearing_stat.rt <- data.frame(mu.hearing.rt, mu.deaf.rt) 
precis(post_hearing_stat.rt)

# test contrast
diff.deaf.hearing.rt <- mu.hearing.rt - mu.deaf.rt
quantile(diff.deaf.hearing.rt, probs = c(0.025, 0.5, 0.975))
```

### Target looking for each age group

Test older vs. younger age group.

```{r age_group acc}
acc_age_cat_targ.m <- map2stan(
    alist(
        mean_prop_looking_TD ~ dnorm(mu, sigma),
        mu <- Intercept + b_Older*age_group_older + b_Adults*age_group_adults,
        Intercept ~ dnorm(0.6, 10), 
        b_Older ~ dnorm(0, 1),
        b_Adults ~ dnorm(0, 1),
        sigma ~ dunif(0, 10)
    ),
    data = select(filter(d_model, value_cat == "Target"), mean_prop_looking_TD, 
                         age_group_adults, age_group_older)
)

post_ages.acc <- extract.samples(acc_age_cat_targ.m)

mu.younger <- post_ages.acc$Intercept
mu.older <- post_ages.acc$b_Older + post_ages.acc$Intercept
mu.adults <- post_ages.acc$b_Adults + post_ages.acc$Intercept
post_df.acc <- data.frame(mu.younger, mu.older, mu.adults)
precis(post_df.acc)

# contrast older vs. younger (target looking)
diff.old.young <- mu.younger - mu.older
quantile(diff.old.young, probs = c(0.025, 0.5, 0.975))
```

### RT for each age group

```{r age_group rt}
rt_age_cat.m <- map2stan(
    alist(
        median_rt ~ dnorm(mu, sigma),
        mu <- Intercept + b_Older*age_group_older + b_Adults*age_group_adults,
        Intercept ~ dnorm(1300, 100), 
        b_Older ~ dnorm(0, 500),
        b_Adults ~ dnorm(0, 500),
        sigma ~ dunif(0, 500)
    ),
    data = select(filter(d_model, value_cat == "Target"), median_rt, age_group_older, age_group_adults)
)

post_ages.rt <- extract.samples(rt_age_cat.m)

mu.younger.rt <- as.numeric(post_ages.rt$Intercept)
mu.older.rt <- as.numeric(post_ages.rt$b_Older + post_ages.rt$Intercept)
mu.adults.rt <- as.numeric(post_ages.rt$b_Adults + post_ages.rt$Intercept)
post_df.rt <- data.frame(mu.younger.rt, mu.older.rt, mu.adults.rt) 
precis(post_df.rt)

diff.old.young.rt <- mu.younger.rt - mu.older.rt
quantile(diff.old.young.rt, probs = c(0.025, 0.5, 0.975))
```

### Target vs. Distractor looking for each age group

```{r}
acc_age_cat_dist.m <- map2stan(
    alist(
        mean_prop_looking_TD ~ dnorm(mu, sigma),
        mu <- Intercept + b_Older*age_group_older + b_Adults*age_group_adults,
        Intercept ~ dnorm(0.6, 10), 
        b_Older ~ dnorm(0, 1),
        b_Adults ~ dnorm(0, 1),
        sigma ~ dunif(0, 10)
    ),
    data = select(filter(d_model, value_cat == "Distractor"), mean_prop_looking_TD, 
                         age_group_adults, age_group_older)
)

# get samples
post_ages.dist <- extract.samples(acc_age_cat_dist.m)

# Distractor: compute averages for each group (intercept is youngest kids)
mu.younger.dist <- post_ages.dist$Intercept
mu.older.dist <- post_ages.dist$b_Older + post_ages.dist$Intercept
mu.adults.dist <- post_ages.dist$b_Adults + post_ages.dist$Intercept
post_df.dist <- data.frame(mu.younger.dist, mu.older.dist, mu.adults.dist)
precis(post_df.dist)

# Merge Target and Distractor looking posterior samples
post_df.dist$looking_type <- "Distractor"
post_df.dist %<>% rename(mu.younger = mu.younger.dist,
                         mu.older = mu.older.dist,
                         mu.adults = mu.adults.dist)
post_df.acc$looking_type <- "Target"
post_df.acc_all <- rbind(post_df.acc, post_df.dist)

# target vs. distractor looking at each time point
diff.targ.dist.young <- mu.younger - mu.younger.dist
quantile(diff.targ.dist.young, probs = c(0.025, 0.5, 0.975))

diff.targ.dist.older <- mu.older - mu.older.dist
quantile(diff.targ.dist.older, probs = c(0.025, 0.5, 0.975))

diff.targ.dist.adults <- mu.adults - mu.adults.dist
quantile(diff.targ.dist.adults, probs = c(0.025, 0.5, 0.975))
```

### Accuracy and RT for kids compared to adults

```{r age_groups_adults acc}
acc_age_cat_targ.m.collapsed <- map2stan(
    alist(
        mean_prop_looking_TD ~ dnorm(mu, sigma),
        mu <- Intercept + b_Adults*age_group_adults_collapsed,
        Intercept ~ dnorm(0.6, 10), 
        b_Adults ~ dnorm(0, 1),
        sigma ~ dunif(0, 10)
    ),
    data = select(filter(d_model, value_cat == "Target"), mean_prop_looking_TD, 
                         age_group_adults_collapsed)
)

post_ages.acc_collapsed <- extract.samples(acc_age_cat_targ.m.collapsed)

mu.kids.collapsed <- post_ages.acc_collapsed$Intercept
mu.adults.collapsed <- post_ages.acc_collapsed$b_Adults + post_ages.acc_collapsed$Intercept
post_df.acc.collapsed <- data.frame(mu.kids.collapsed, mu.adults.collapsed)
precis(post_df.acc.collapsed)

diff.old.young.acc.collapsed <- mu.kids.collapsed - mu.adults.collapsed
quantile(diff.old.young.acc.collapsed, probs = c(0.025, 0.5, 0.975))
```

```{r age_group_adults rt}
rt_age_cat_collapsed.m <- map2stan(
    alist(
        median_rt ~ dnorm(mu, sigma),
        mu <- Intercept + b_Adults*age_group_adults_collapsed,
        Intercept ~ dnorm(1300, 100), 
        b_Adults ~ dnorm(0, 500),
        sigma ~ dunif(0, 500)
    ),
    data = select(filter(d_model, value_cat == "Target"), median_rt, age_group_adults_collapsed)
)

post_ages.rt_collapsed <- extract.samples(rt_age_cat_collapsed.m)

mu.kids.rt.collapsed <- as.numeric(post_ages.rt_collapsed$Intercept)
mu.adults.rt.collapsed <- as.numeric(post_ages.rt_collapsed$b_Adults + post_ages.rt_collapsed$Intercept)
post_df.rt.collapsed <- data.frame(mu.kids.rt.collapsed, mu.adults.rt.collapsed) 
precis(post_df.rt.collapsed)

diff.old.young.rt.collapsed <- mu.kids.rt.collapsed - mu.adults.rt.collapsed
quantile(diff.old.young.rt.collapsed, probs = c(0.025, 0.5, 0.975))
```

### Bayes Factor Analysis for CODA vs. Deaf comparison

From Lee and Wagenmakers Ch.8 on two-sample comparisons

"In our Bayesian approach of making inferences about the means, we rescale the
data so that one group has mean 0 and standard deviation 1. This rescaling procedure
ensures that the prior distributions for the parameters hold regardless of the
scale of measurement. Therefore it does not matter whether, say, response times
are measured in seconds or in milliseconds."

```{r hearing_status acc bf}
# get accuracy data
x <- d %>% 
    filter(value_cat == "Target", age_group_collapsed == "Kids", 
           hearing_status_participant == "deaf") %>% 
    select(mean_prop_looking_TD) 

y <- d %>% 
    filter(value_cat == "Target", age_group_collapsed == "Kids", 
           hearing_status_participant == "hearing") %>% 
    select(mean_prop_looking_TD)

x <- x$mean_prop_looking_TD
y <- y$mean_prop_looking_TD

n1 <- length(x)
n2 <- length(y)

# Rescale data 
y <- y - mean(x)
y <- y/sd(x)
x <- (x-mean(x))/sd(x); data <- list("x", "y", "n1", "n2") # to be passed on to JAGS

myinits <- list(
    list(delta = rnorm(1,0,3), mu = rnorm(1,0,1), sigmatmp = runif(1,0,5)),
    list(delta = rnorm(1,0,3), mu = rnorm(1,0,1), sigmatmp = runif(1,0,5)),
    list(delta = rnorm(1,0,3), mu = rnorm(1,0,1), sigmatmp = runif(1,0,5)))

# Parameters to be monitored
parameters <- c("delta")

# The following command calls JAGS with specific options.
# For a detailed description see the R2jags documentation.
samples <- jags(data, inits=myinits, parameters,
                model.file = "categorical_model.txt",
                n.chains=3, n.iter=10000, n.burnin=5000, n.thin=1, DIC=T)

df <- data.frame(delta.posterior=samples$BUGSoutput$sims.list$delta)

# Fits a density using spliens to approx. log-density
# uses 1997 knot and deletion algorithm
fit.posterior<- logspline(df$delta.posterior) #NEW

# 95% CI for log-density (of posterior of effect size)
ci.low<-qlogspline(0.025,fit.posterior)
ci.hi<-qlogspline(0.975,fit.posterior)

posterior<-dlogspline(0, fit.posterior) # pdf @ delta=0
prior<-dcauchy(0) # height of order--restricted prior
BF01<-posterior/prior # bayes factor
BF01

df$delta.prior<-rcauchy(length(df$delta.posterior),0,1)

ggplot(data=df)+
    geom_density(aes(x=delta.posterior),linetype='dashed')+
    geom_density(aes(x=delta.prior))+
    geom_point(x=0,y=prior,color='red')+
    geom_point(x=0,y=posterior,color='blue')+
    theme_bw()+
    xlim(-3,3)+
    #ylim(0,10)+
    xlab('Delta')
```

```{r hearing_status rt bf}
# get accuracy data
x <- d %>% 
    filter(value_cat == "Target", age_group_collapsed == "Kids", 
           hearing_status_participant == "deaf") %>% 
    select(median_rt) 

y <- d %>% 
    filter(value_cat == "Target", age_group_collapsed == "Kids", 
           hearing_status_participant == "hearing") %>% 
    select(median_rt)

x <- x$median_rt
y <- y$median_rt

n1 <- length(x)
n2 <- length(y)

# Rescale data 
y <- y - mean(x)
y <- y/sd(x)
x <- (x-mean(x))/sd(x); data <- list("x", "y", "n1", "n2") # to be passed on to JAGS

myinits <- list(
    list(delta = rnorm(1,0,3), mu = rnorm(1,0,1), sigmatmp = runif(1,0,5)),
    list(delta = rnorm(1,0,3), mu = rnorm(1,0,1), sigmatmp = runif(1,0,5)),
    list(delta = rnorm(1,0,3), mu = rnorm(1,0,1), sigmatmp = runif(1,0,5)))

# Parameters to be monitored
parameters <- c("delta")

# The following command calls JAGS with specific options.
# For a detailed description see the R2jags documentation.
samples <- jags(data, inits=myinits, parameters,
                model.file = "categorical_model.txt",
                n.chains=3, n.iter=10000, n.burnin=5000, n.thin=1, DIC=T)

df <- data.frame(delta.posterior=samples$BUGSoutput$sims.list$delta)

# Fits a density using spliens to approx. log-density
# uses 1997 knot and deletion algorithm
fit.posterior<- logspline(df$delta.posterior) 

# 95% CI for log-density (of posterior of effect size)
ci.low<-qlogspline(0.025,fit.posterior)
ci.hi<-qlogspline(0.975,fit.posterior)

posterior<-dlogspline(0, fit.posterior) # pdf @ delta=0
prior<-dcauchy(0) # height of order--restricted prior
BF01<-posterior/prior # bayes factor
BF01

df$delta.prior<-rcauchy(length(df$delta.posterior),0,1)

ggplot(data=df)+
    geom_density(aes(x=delta.posterior),linetype='dashed')+
    geom_density(aes(x=delta.prior))+
    geom_point(x=0,y=prior,color='red')+
    geom_point(x=0,y=posterior,color='blue')+
    theme_bw()+
    xlim(-3,3)+
    #ylim(0,10)+
    xlab('Delta')
```

### Summary barplots for categorical analyses

```{r barplot}
# Accuracy: MAP and HDI for each age group
post_df.acc_all %<>% gather(key = age_group, value = post_sample, mu.younger:mu.adults) 

ms_all <- post_df.acc_all %>%
    group_by(age_group, looking_type) %>% 
    summarise(mean_post = mean(post_sample),
              HDI_lower = HDIofMCMC(post_sample, credMass = 0.95)[1],
              HDI_upper = HDIofMCMC(post_sample, credMass = 0.95)[2]) %>% 
    mutate(HDI_lower = ifelse(HDI_lower <= 0, 0, HDI_lower),
           looking_type = ifelse(looking_type == "Distractor", "Distracter", "Target")) 

######### Now plot bar graph of MAP and HDI for target/distractor looking ############

# RT: MAP and HDI for each age group
post_df.rt %<>% gather(key = age_group, value = post_sample)

ms_all_rt <- post_df.rt %>%
    group_by(age_group) %>% 
    summarise(mean_post = mean(post_sample),
              HDI_lower = HDIofMCMC(post_sample, credMass = 0.95)[1],
              HDI_upper = HDIofMCMC(post_sample, credMass = 0.95)[2]) 

# Bar Plot
all_acc <- ggplot(aes(x = age_group, y = mean_post, fill = looking_type), data = ms_all) +
    geom_bar(stat = "identity", position="dodge") +
    geom_linerange(aes(ymin = HDI_lower,
                       ymax = HDI_upper),
                   position = position_dodge(width = 0.9)) +
    xlab("Age Group") +
    ylab("Proportion Looking") +
    coord_cartesian(ylim=c(0, 1)) +
    scale_fill_brewer(type = "qual", palette = "Set1") +
    guides(fill=guide_legend(title=NULL)) +
    scale_x_discrete(labels=c("Younger", "Older", "Adults")) +
    theme_bw() +
    theme(plot.title = element_text(face = "bold", size = 20),
          axis.title.x = element_text(colour="grey40",size=18,
                                      angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.title.y = element_text(colour="grey40",size=18,
                                      angle=90,hjust=0.5,vjust=0.5,face="plain"),
          axis.text.x = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.text.y = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          plot.margin = unit(c(0.5,2,1,1), "cm"),
          legend.position = c(0.3, 0.85),
          legend.text = element_text(size = 16)) +
    ggtitle("Accuracy") 

all_rt <- ggplot(aes(x= age_group, y = mean_post, fill = age_group), data = ms_all_rt) +
    geom_bar(stat = "identity") +
    geom_linerange(aes(ymax = HDI_upper, ymin = HDI_lower)) +
    scale_fill_brewer(type = "qual", palette = "Set1") +
    guides(fill=F) +
    xlab("Age Group") +
    ylab("Mean RT (ms)") +
    scale_x_discrete(labels=c("Younger", "Older", "Adults")) +
    ylim(0, 1600) +
    theme_bw() +
    theme(plot.title = element_text(face = "bold", size = 20),
          axis.title.x = element_text(colour="grey40",size=18,
                                      angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.title.y = element_text(colour="grey40",size=18,
                                      angle=90,hjust=0.5,vjust=0.5,face="plain"),
          axis.text.x = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.text.y = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          plot.margin = unit(c(0.5,2,1,1), "cm")) +
    ggtitle("Reaction Time") 

gridExtra::grid.arrange(all_acc, all_rt, ncol = 2)

```

## Univariate regressions predicting processing measures 

All models fit using JAGS and based on models from Ch. 17 in @kruschke2014doing. 

First, we need to set the parameters for the MCMC simulations. This will be the same 
across all models. The key parameter of interest is the standardized slope: zbeta1.
This encodes the strength of the linear relationship between the predictor variable
and the outcome.

```{r set parameters}
parameters = c( "zbeta0" , "zbeta1" , "zsigma" )
adaptSteps = 500  # Number of steps to "tune" the samplers
burnInSteps = 1000
nChains = 1 
thinSteps = 1
numSavedSteps = 50000
nIter = ceiling( ( numSavedSteps * thinSteps ) / nChains )
```

### Age and accuracy

First, we take samples from the prior predictive distribution to show that we are considering 
a plausible set of paraemter values for the slope and interecept. Here we are using our prior knowledge
from past work with spoken language showing that expected change in accuracy for each month of age is ~.0167, or 1.6% points. 

Draw samples from the prior.

```{r}
# to sample from prior we do not condition on the observed data
dataList_age_prior = list(
    #y = d$acc.s,
    x = d$age.s,
    Ntotal = length(d$age.s)
)

samples_prior <- jags(data = dataList_age_prior, parameters.to.save = parameters,
                      model.file="accuracy_model.txt", n.chains=nChains, 
                      n.iter=nIter, n.burnin = burnInSteps,
                      n.thin=1, DIC=F)

df_prior <- data.frame(zbeta0 = samples_prior$BUGSoutput$sims.list$zbeta0, 
                       zbeta1 = samples_prior$BUGSoutput$sims.list$zbeta1)

# transform standardized prior to measurement scale
df_prior %<>% 
    mutate( beta1 = zbeta1 * sd(d$mean_prop_looking_TD) / sd(d$age_peek_months),
            beta0 = zbeta0 * sd(d$mean_prop_looking_TD) + mean(d$mean_prop_looking_TD) - (zbeta1 * mean(d$age_peek_months) * sd(d$mean_prop_looking_TD)) / sd(d$age_peek_months)
    )
```

Now we can plot the prior distribution of the slope parameter (beta1) on the measurement scale. 

```{r}
ggplot() + geom_density(aes(df_prior$beta1)) +
    xlab("Beta 1") +
    ylab("Density") +
    ggtitle("Prior Predictive on Slope Parameter")
```

We can see that the prior model puts most of the density on a value of 0, or a null relationship between
age and accuracy. It is truncated at zero to eoncode our directional hypothesis: that the slope parameter
should be positive. And the prior does not consider extreme slope values beyond .04, or a 4% gain in accuracy with each month. 

Now that we are confident that our prior distribution produces reasonable hypotheses, we can 
condition on the data and quantify the association between children's age and accuracy on the VLP task. Conditioned on the data, we draw samples from the posterior.

```{r}
dataList_age = list(
    x = d$age.s,
    y = d$acc.s,
    Ntotal = length(d$age.s)
)

samples <- jags(data = dataList_age, parameters.to.save = parameters,
                model.file="accuracy_model.txt", n.chains=nChains, 
                n.iter=nIter, n.burnin = burnInSteps,
                n.thin=1, DIC=T)

df <- data.frame(zbeta0 = samples$BUGSoutput$sims.list$zbeta0, 
                 zbeta1 = samples$BUGSoutput$sims.list$zbeta1)

# transform standardized coefs to measurement scale coefs
df %<>% mutate(
    beta1 = zbeta1 * sd(d$mean_prop_looking_TD) / sd(d$age_peek_months),
    beta0 = zbeta0 * sd(d$mean_prop_looking_TD) + mean(d$mean_prop_looking_TD) - zbeta1 * mean(d$age_peek_months) * sd(d$mean_prop_looking_TD) / sd(d$age_peek_months)
)
```

The first question we ask is whether there is evidence of a linear relationship to measure. We answer this
question by using the Savage-Dickey method for estimating the Bayes Factor. This method uses the statistical fact that the Bayes factor is the ratio of the prior and posterior at the point in the parameter space that reduces the full model to the nested model.

Thus, we treat the null model of no association between age and accuracy as a special case of the
linear model where the slope parameter equals 0. 

```{r}
# Compute p(D) for posterior and prior: Savage-Dickey Method to get Bayes Factor
# Fits a density using splines to approx. log-density
# uses 1997 knot and deletion algorithm
fit.posterior <- logspline(df$zbeta1, lbound = 0)
posterior <- dlogspline(0, fit.posterior) # pdf @ beta=0

# get prior density using logspline
fit.prior <- logspline(df_prior$zbeta1, lbound = 0)
prior <- dlogspline(0, fit.prior) # pdf @ beta=0

ggplot(data=df) +
    geom_density(aes(x=zbeta1), linetype='dashed') +
    geom_density(aes(x=zbeta1), data = df_prior) +
    xlim(0,2) +
    xlab("Standardized Beta")
```

We get the Bayes Factor by taking the ratio of the density of the posterior and prior when the slope
parameter is zero. 

```{r}
# bayes factor
bf_acc_age <- posterior/prior
1/bf_acc_age
```

The prior and posterior are shown, and the posterior is about 12 times lower than the prior at the critical
point beta = 0. This means that the linear model is 12 times more likely than the null, intercept-only model. 

Now that we know there is evidence of a linear association, we can summarize the strength of the
association and the uncertainty around plausible values for slope parameter.

```{r}
post_mode_b1_acc_age <- round(find_mode(df$beta1), 3)
post_mode_b0_acc_age <- round(find_mode(df$beta0), 3)
HDI_b1_acc_age <- HDIofMCMC(df$beta1 , credMass = 0.95 )

## simulate posterior distribution of mu for each value of age to get HDI for MAP regression line
mu.link <- function(x) {df$beta0 + df$beta1*x}
age.seq <- seq(from = 10, to = 60, by = 1)
mu <- sapply(age.seq, mu.link)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.95)
mu.HPDI_tidy <- data.frame(age_peek_months = age.seq, 
                           hpdi_lower = mu.HPDI[1,], hpdi_upper = mu.HPDI[2,],
                           mu.mean = mu.mean)

ggplot(data = d) +
    geom_line(aes(x = age_peek_months, y = mu.mean), data = mu.HPDI_tidy, size = 2) +
    geom_point(aes(age_peek_months, mean_prop_looking_TD), color = "black", size = 5.5) +
    geom_point(aes(age_peek_months, mean_prop_looking_TD), color = "grey50", size = 4) +
    geom_ribbon(aes(x = age_peek_months, ymin = hpdi_lower, ymax = hpdi_upper), data = mu.HPDI_tidy,
                alpha = 0.2) +
    ylab("Accuracy") +
    xlab("Child's Age (months)") +
    coord_cartesian(xlim=c(15, 55), ylim=c(0.25, 0.95)) +
    ggtitle(bquote(list(beta==.(as.character(round(post_mode_b1_acc_age, 3))),
                        HDI=.(paste("95% HDI [", round(HDI_b1_acc_age[1],3) , "," , 
                                     round(HDI_b1_acc_age[2], 3), "]"))))) +
    theme(plot.title = element_text(size = 20),
        axis.title.x = element_text(colour="grey30",size=22,
                                      angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.title.y = element_text(colour="grey30",size=22,
                                      hjust=0.5,vjust=0.5,face="plain"),
          axis.text.x = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.text.y = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          panel.grid.major=element_blank())
```

### Vocab and accuracy 

Draw samples from the prior.

```{r}
dataList_voc_prior = list(
    #y = d_voc$mean_prop_looking_TD,
    x = d_voc$voc.s,
    Ntotal = length(d_voc$age.s)
)

samples_prior <- jags(data = dataList_voc_prior, parameters.to.save = parameters,
                      model.file="accuracy_model.txt", n.chains=nChains, 
                      n.iter=nIter, n.burnin = burnInSteps,
                      n.thin=1, DIC=F)

df_prior <- data.frame(zbeta0 = samples_prior$BUGSoutput$sims.list$zbeta0, 
                       zbeta1 = samples_prior$BUGSoutput$sims.list$zbeta1)

# transform standardized prior to measurement scale
df_prior %<>% 
    mutate( beta1 = zbeta1 * sd(d_voc$mean_prop_looking_TD) / sd(d_voc$signs_produced),
            beta0 = zbeta0 * sd(d_voc$mean_prop_looking_TD) + mean(d_voc$signs_produced) - (zbeta1 * mean(d_voc$signs_produced) * sd(d_voc$mean_prop_looking_TD)) / sd(d_voc$signs_produced)
    )
```

Draw samples from the posterior.

```{r}
dataList_voc = list(
    x = d_voc$voc.s,
    y = d_voc$acc.s,
    Ntotal = length(d_voc$age.s)
    
)
samples <- jags(data = dataList_voc, parameters.to.save = parameters,
                model.file="accuracy_model.txt", n.chains=nChains, n.iter=nIter, 
                n.thin=1, DIC=T, n.burnin=burnInSteps)

df <- data.frame(zbeta0 = samples$BUGSoutput$sims.list$zbeta0, 
                 zbeta1 = samples$BUGSoutput$sims.list$zbeta1)

# transform standardized prior to measurement scale
df %<>%
    mutate( beta1 = zbeta1 * sd(d_voc$mean_prop_looking_TD) / sd(d_voc$signs_produced),
            beta0 = zbeta0 * sd(d_voc$mean_prop_looking_TD) + mean(d_voc$mean_prop_looking_TD) - zbeta1 * mean(d_voc$signs_produced) * sd(d_voc$mean_prop_looking_TD) / sd(d_voc$signs_produced)
    )
```

We get the Bayes Factor by taking the ratio of the density of the posterior and prior when the slope
parameter is zero. 

```{r}
fit.posterior <- logspline(df$beta1, lbound = 0)
posterior <- dlogspline(0, fit.posterior) # pdf @ beta=0

# get prior density using logspline
fit.prior <- logspline(df_prior$beta1, lbound = 0)
prior <- dlogspline(0, fit.prior) # pdf @ beta=0

ggplot(data=df) +
    geom_density(aes(x=zbeta1), linetype='dashed') +
    geom_density(aes(x=zbeta1), data = df_prior) +
    xlim(0,2) +
    xlab("Standardized Beta")
```

```{r}
bf_acc_voc <- posterior/prior # bayes factor
1/bf_acc_voc
```

Now that we know there is evidence of a linear association, we can summarize the strength of the
association and the uncertainty around plausible values for slope parameter.

```{r}
## get MAP and HDI on slope and intercept
post_mode_b1_accvoc <- round(find_mode(df$beta1), 3)
post_mode_b0_accvoc <- round(find_mode(df$beta0), 3)
HDI_b1_accvoc <- HDIofMCMC(df$beta1 , credMass = 0.95 )

## simulate to get HDI for regression line
vocab.seq <- seq(from = 15, to = 85, by = 1)
mu.link <- function(x) {df$beta0 + df$beta1*x}
mu <- sapply(vocab.seq, mu.link)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.95)
mu.HPDI_tidy <- data.frame(signs_produced = vocab.seq, hpdi_lower = mu.HPDI[1,], hpdi_upper = mu.HPDI[2,])

ggplot(data = d_voc) +
    geom_line(aes(x = signs_produced, y = mu.mean), data = mu.HPDI_tidy, size = 2) +
    geom_point(aes(signs_produced, mean_prop_looking_TD), color = "black", size = 5.5) +
    geom_point(aes(signs_produced, mean_prop_looking_TD), color = "grey50", size = 4) +
    geom_ribbon(aes(x = signs_produced, ymin = hpdi_lower, ymax = hpdi_upper), data = mu.HPDI_tidy,
                alpha = 0.2) +
    ylab("Accuracy") +
    xlab("Signs Produced") +
    coord_cartesian(xlim=c(18, 82), ylim=c(0.25, 0.95)) +
    ggtitle(bquote(list(beta==.(as.character(round(post_mode_b1_accvoc, 3))),
                        HDI=.(paste("95% HDI [", round(HDI_b1_accvoc[1],3) , "," , 
                                    round(HDI_b1_accvoc[2], 3), "]"))))) +
    theme(plot.title = element_text(size = 20),
        axis.title.x = element_text(colour="grey30",size=22,
                                      angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.title.y = element_text(colour="grey30",size=22,
                                      hjust=0.5,vjust=0.5,face="plain"),
          axis.text.x = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.text.y = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          panel.grid.major=element_blank())
```

### Age and RT 

Draw samples from the prior.

```{r}
# Specify the data in a list, for later shipment to JAGS:
dataList_prior = list(
    x = d$age.s,
    #y = d$rt.s,
    n_correct = d$C_T_count,
    n_trials = d$total_trials_shifting,
    Ntotal = length(d$acc.s)
)

parameters = c( "true_beta0" , "true_beta1" , "zbeta0")

myinits <-  list(list(phi = 0.75, z = round(runif(length(d$total_trials_shifting)))))

samples_prior <- jags(data = dataList_prior, parameters.to.save = parameters,
                model.file="rt_model.txt", n.chains=nChains, 
                n.iter=nIter, n.burnin = burnInSteps, inits = myinits,
                n.thin=1, DIC=T)


df_rt_age_prior <- data.frame(zbeta0 = samples_prior$BUGSoutput$sims.list$true_beta0,
                        zbeta1 = samples_prior$BUGSoutput$sims.list$true_beta1)

## transform to measurement scale (RT in ms)
df_rt_age_prior %<>% mutate(
    beta1 = zbeta1 * sd(d$median_rt) / sd(d$age_peek_months),
    beta0 = zbeta0 * sd(d$median_rt) + mean(d$median_rt) - zbeta1 * mean(d$age_peek_months) * sd(d$median_rt) / sd(d$age_peek_months)
)
```

First, we take samples from the prior predictive distribution to show that we are considering 
a plausible set of parameter values for the slope and interecept. Here we use past research on spoken language to inform our prior: that expected change in RT for each month of age is ~33ms.

```{r}
ggplot() + geom_density(aes(df_rt_age_prior$beta1)) +
    xlab("Beta 1") +
    ylab("Density") +
    ggtitle("Prior Predictive on Slope Parameter")
```

By inspecting the prior distribution we can see that the model makes reasonable predictions for the slope parameter. The distribution is centered around zero, conservatively predicting no association between age 
and RT. The value from prior data of 33 ms is considered plausible, and extends to extreme values of triple that value.

Draw samples from posterior. 

```{r}
dataList_post = list(
    x = d$age.s,
    y = d$rt.s,
    n_correct = d$C_T_count,
    n_trials = d$total_trials_shifting,
    Ntotal = length(d$acc.s)
)

parameters = c( "true_beta0" , "true_beta1" , "phi" , "z")

myinits <-  list(list(phi = 0.75, z = round(runif(length(d$total_trials_shifting)))))

samples_post <- jags(data = dataList_post, parameters.to.save = parameters,
                model.file="rt_model.txt", n.chains=nChains, inits = myinits,
                n.iter=nIter, n.burnin = burnInSteps,
                n.thin=1, DIC=T)

df_rt_age_post <- data.frame(zbeta0 = samples_post$BUGSoutput$sims.list$true_beta0,
                        zbeta1 = samples_post$BUGSoutput$sims.list$true_beta1,
                        z = samples_post$BUGSoutput$sims.list$z)

## transform to measurement scale (RT in ms)
df_rt_age_post %<>% mutate(
    beta1 = zbeta1 * sd(d$median_rt) / sd(d$age_peek_months),
    beta0 = zbeta0 * sd(d$median_rt) + mean(d$median_rt) - zbeta1 * mean(d$age_peek_months) * sd(d$median_rt) / sd(d$age_peek_months)
)
```

Plot prior with posterior.

```{r}
ggplot(data=df_rt_age_post) +
    geom_density(aes(x=zbeta1), linetype='dashed') +
    geom_density(aes(x=zbeta1), data = df_rt_age_prior) +
    xlim(-1, 0) +
    xlab("Standardized Beta")
```

Get Bayes Factor using Savage-Dickey method.

```{r}
# Compute p(D) for posterior and prior: Savage-Dickey Method to get Bayes Factor
# Fits a density using splines to approx. log-density
# uses 1997 knot and deletion algorithm
fit.posterior <- logspline(df_rt_age_post$zbeta1, ubound = 0)
posterior <- dlogspline(0, fit.posterior) # pdf @ beta=0

# get prior density using logspline
fit.prior <- logspline(df_rt_age_prior$zbeta1, ubound = 0)
prior <- dlogspline(0, fit.prior) # pdf @ beta=0

bf_rt_age <- posterior / prior
1/bf_rt_age
```

The data is 1.14 times as likely under the linear model. This is the weakest evidence among all four linear
associations. 

We can still estimate the slope parameter, despite the small Bayes Factor.

```{r}
HDI_rt_age_b1 <- HDIofMCMC(df_rt_age_post$beta1 , credMass = 0.95 )
post_rt_age_mode_b1 <- round(find_mode(df_rt_age_post$beta1), 3)
post_rt_age_mode_b0 <- round(find_mode(df_rt_age_post$beta0), 3)

# simulate to get HDI around MAP regression line
mu.link <- function(x) {df_rt_age_post$beta0 + df_rt_age_post$beta1*x}
age.seq <- seq(from = 10, to = 60, by = 1)
mu <- sapply(age.seq, mu.link)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.95)
mu.HPDI_tidy <- data.frame(age_peek_months = age.seq, 
                           hpdi_lower = mu.HPDI[1,], hpdi_upper = mu.HPDI[2,],
                           mu.rt = mu.mean)

ggplot(data = d) +
    geom_line(aes(x = age_peek_months, y = mu.rt), data = mu.HPDI_tidy, size = 2) +
    geom_point(aes(age_peek_months, median_rt), color = "black", size = 5.5) +
    geom_point(aes(age_peek_months, median_rt), color = "grey50", size = 4) +
    geom_ribbon(aes(x = age_peek_months, ymin = hpdi_lower, ymax = hpdi_upper), data = mu.HPDI_tidy,
                alpha = 0.2) +
    ylab("Reaction Time (ms)") +
    xlab("Child's Age (months)") +
    coord_cartesian(xlim=c(15, 55), ylim=c(700, 2000)) +
    ggtitle(bquote(list(beta==.(as.character(round(post_rt_age_mode_b1, 2))),
                        HDI=.(paste("95% HDI [", round(HDI_rt_age_b1[1],3) , "," , 
                                    round(HDI_rt_age_b1[2], 3), "]"))))) +
    theme(plot.title = element_text(size = 18),
        axis.title.x = element_text(colour="grey30",size=22,
                                      angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.title.y = element_text(colour="grey30",size=22,
                                      hjust=0.5,vjust=0.5,face="plain"),
          axis.text.x = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.text.y = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          panel.grid.major=element_blank())
```

### Vocab and RT 

Draw samples from prior.

```{r}
dataList_prior = list(
    x = d_voc$voc.s,
    #y = d_voc$rt.s,
    n_correct = d_voc$C_T_count,
    n_trials = d_voc$total_trials_shifting,
    Ntotal = length(d_voc$voc.s)
)

parameters = c( "true_beta0" , "true_beta1" )

myinits <-  list(list(phi = 0.75, z = round(runif(length(d_voc$total_trials_shifting)))))

samples_prior <- jags(data = dataList_prior, parameters.to.save = parameters,
                model.file="rt_model.txt", n.chains=nChains, inits = myinits,
                n.iter=nIter, n.burnin = burnInSteps,
                n.thin=1, DIC=F)


df_rt_voc_prior <- data.frame(zbeta0 = samples_prior$BUGSoutput$sims.list$true_beta0,
                        zbeta1 = samples_prior$BUGSoutput$sims.list$true_beta1)

## transform to measurement scale (RT in ms)
df_rt_voc_prior %<>% mutate(
    beta1 = zbeta1 * sd(d_voc$median_rt) / sd(d_voc$signs_produced),
    beta0 = zbeta0 * sd(d_voc$median_rt) + mean(d_voc$median_rt) - zbeta1 * mean(d_voc$signs_produced) * sd(d_voc$median_rt) / sd(d_voc$signs_produced)
)
```

```{r}
ggplot() + geom_density(aes(df_rt_voc_prior$beta1)) +
    xlab("Beta 1") +
    ylab("Density") +
    ggtitle("Prior Predictive on Slope Parameter")
```

Draw samples from posterior.

```{r}
dataList_post = list(
    x = d_voc$voc.s,
    y = d_voc$rt.s,
    n_correct = d_voc$C_T_count,
    n_trials = d_voc$total_trials_shifting,
    Ntotal = length(d_voc$voc.s)
)


parameters = c( "true_beta0" , "true_beta1" , "z")
myinits <-  list(list(phi = 0.75, z = round(runif(length(d_voc$total_trials_shifting)))))

samples_post <- jags(data = dataList_post, parameters.to.save = parameters,
                model.file="rt_model.txt", n.chains=nChains, inits = myinits,
                n.iter=nIter, n.burnin = burnInSteps,
                n.thin=1, DIC=T)

df_rt_voc_post <- data.frame(zbeta0 = samples_post$BUGSoutput$sims.list$true_beta0,
                        zbeta1 = samples_post$BUGSoutput$sims.list$true_beta1,
                        z = samples_post$BUGSoutput$sims.list$z)

## transform to measurement scale (RT in ms)
df_rt_voc_post %<>% mutate(
    beta1 = zbeta1 * sd(d_voc$median_rt) / sd(d_voc$signs_produced),
    beta0 = zbeta0 * sd(d_voc$median_rt) + mean(d_voc$median_rt) - zbeta1 * mean(d_voc$signs_produced) * sd(d_voc$median_rt) / sd(d_voc$signs_produced)
)
```

Plot prior with posterior.

```{r}
ggplot(data=df_rt_voc_post) +
    geom_density(aes(x=beta1), linetype='dashed') +
    geom_density(aes(x=beta1), data = df_rt_voc_prior) +
    xlim(-20, 0) +
    xlab("RT Beta")
```

Get Bayes Factor.

```{r}
# Compute p(D) for posterior and prior: Savage-Dickey Method to get Bayes Factor
# Fits a density using splines to approx. log-density
# uses 1997 knot and deletion algorithm
fit.posterior <- logspline(df_rt_voc_post$zbeta1, ubound = 0)
posterior <- dlogspline(0, fit.posterior) # pdf @ beta=0

# get prior density using logspline
fit.prior <- logspline(df_rt_voc_prior$zbeta1, ubound = 0)
prior <- dlogspline(0, fit.prior) # pdf @ beta=0

bf_rt_voc <- posterior / prior
1/bf_rt_voc
```

Estimate strength of association.

```{r}
HDI_rt_voc_b1 <- HDIofMCMC( df_rt_voc_post$beta1 , credMass = 0.95 )
post_mode_b1_rt_voc <- round(find_mode(df_rt_voc_post$beta1), 3)
post_mode_b0_rt_voc <- round(find_mode(df_rt_voc_post$beta0), 3)


vocab.seq <- seq(from = 15, to = 85, by = 1)
mu.link <- function(x) {df_rt_voc_post$beta0 + df_rt_voc_post$beta1*x}
mu <- sapply(vocab.seq, mu.link)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.95)
mu.HPDI_tidy <- data.frame(signs_produced = vocab.seq, hpdi_lower = mu.HPDI[1,], hpdi_upper = mu.HPDI[2,],
                           mu.rt = mu.mean)

# plot
ggplot(data = d_voc) +
    geom_line(aes(x = signs_produced, y = mu.mean), data = mu.HPDI_tidy, size = 2) +
    geom_point(aes(signs_produced, median_rt), color = "black", size = 5.5) +
    geom_point(aes(signs_produced, median_rt), color = "grey50", size = 4) +
    geom_ribbon(aes(x = signs_produced, ymin = hpdi_lower, ymax = hpdi_upper), data = mu.HPDI_tidy,
                alpha = 0.2) +
    ylab("Reaction Time (ms)") +
    xlab("Signs Produced") +
    coord_cartesian(xlim=c(18, 82), ylim=c(700, 2000)) +
   ggtitle(bquote(list(beta==.(as.character(round(post_mode_b1_rt_voc, 2))),
                        HDI=.(paste("95% HDI [", round(HDI_rt_voc_b1[1],3) , "," , 
                                     round(HDI_rt_voc_b1[2], 3), "]")))))  +
    theme(plot.title = element_text(size=20),
        axis.title.x = element_text(colour="grey30",size=20,
                                      angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.title.y = element_text(colour="grey30",size=22,
                                      hjust=0.5,vjust=0.5,face="plain"),
          axis.text.x = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.text.y = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          panel.grid.major=element_blank())

```

### Table of model summaries

```{r}
models <- c("Acc ~ Age", "Acc ~ Voc", "RT ~ Age", "RT ~ Voc")
bfs <- c(1/bf_acc_age, 1/bf_acc_voc, 1/bf_rt_age, 1/bf_rt_voc)
bmaps <- c(post_mode_b1_acc_age, post_mode_b1_accvoc, post_rt_age_mode_b1, post_mode_b1_rt_voc)
hdi_lower <- c(HDI_b1_acc_age[1], HDI_b1_accvoc[1], HDI_rt_age_b1[1], HDI_rt_voc_b1[1])
hdi_upper <- c(HDI_b1_acc_age[2], HDI_b1_accvoc[2], HDI_rt_age_b1[2], HDI_rt_voc_b1[2])

table1 <- data.frame(models, bfs, bmaps, hdi_lower, hdi_upper)

knitr::kable(table1)
```

### Output of latent mixture model inferring guessing probability

Here we show the stability of our inferences about participants' guessing behavior 
across the three different implementations of the latent mixture models:

* Without linear regression
* In the RT ~ Age model
* In the RT ~ Vocab model

First, we run latent mixture model by itself not conditioning on RT or accuracy data.

```{r}
dataList_lat_mix = list(
    n_correct = d$C_T_count,
    n_trials = d$total_trials_shifting,
    Ntotal = length(d$acc.s)
)

myinits <-  list(list(phi = 0.75, z = round(runif(length(d$total_trials_shifting)))))

parameters <- c("z")

samples_post <- jags(data = dataList_lat_mix, parameters.to.save = parameters,
                model.file="latent_mixture.txt", n.chains=nChains, inits = myinits,
                n.iter=nIter, n.burnin = burnInSteps,
                n.thin=1, DIC=T)

df_latent_mix <- data.frame(z = samples_post$BUGSoutput$sims.list$z)

# grab subject numbers and add to data frame
colnames(df_latent_mix)[which(names(df_latent_mix)=="z.1"):which(names(df_latent_mix)=="z.29")] <- as.character(d$Sub.Num)

# melt data frame
df_melt <- reshape::melt.data.frame(df_latent_mix[,2:length(df_latent_mix)], variable.name = "Sub.Num", value.name = "group_membership")

# change factor label
df_melt %<>% mutate(group_membership_factor = factor(df_melt$value, labels = c("G", "K")),
                    model = "latent_mix") 
```

Now do the same thing but for the RT ~ Age model.

```{r}
df_latent_mix <- select(df_rt_age_post, z.1:z.29)

# grab subject numbers and add to data frame
colnames(df_latent_mix)[which(names(df_latent_mix)=="z.1"):which(names(df_latent_mix)=="z.29")] <- as.character(d$Sub.Num)

# melt data frame
df_melt_rt_age <- reshape::melt.data.frame(df_latent_mix[,2:length(df_latent_mix)], variable.name = "Sub.Num", value.name = "group_membership")

# change factor label
df_melt_rt_age %<>% mutate(group_membership_factor = factor(df_melt_rt_age$value, labels = c("G", "K")),
                           model = "rt_age") 
```

Now do the same thing but for the RT ~ Vocab model.

```{r}
df_latent_mix <- select(df_rt_voc_post, z.1:z.28)

# grab subject numbers and add to data frame
colnames(df_latent_mix)[which(names(df_latent_mix)=="z.1"):which(names(df_latent_mix)=="z.28")] <- as.character(d_voc$Sub.Num)

# melt data frame
df_melt_rt_voc <- reshape::melt.data.frame(df_latent_mix[,2:length(df_latent_mix)], variable.name = "Sub.Num", value.name = "group_membership")

# change factor label
df_melt_rt_voc %<>% mutate(group_membership_factor = factor(df_melt_rt_voc$value, labels = c("G", "K")),
                           model = "rt_voc") 
```

Once we have all the simulation data, we can check the stability of the latent mixture model across the three different analyses.

```{r}
df_all_sims <- rbind(df_melt, df_melt_rt_voc, df_melt_rt_age)

## summarise
ms_all_sims <- df_all_sims %>% 
    group_by(variable, model) %>% 
    summarise(prop_guessing = (length(value) - sum(value)) / length(value)) %>% 
    mutate(prop_guessing_convert = prop_guessing * .1)

ggplot(data=ms_all_sims,aes(x=model, y = prop_guessing)) +
    geom_bar(stat="identity") +
    facet_wrap(~variable,scales='fixed') +
    guides(fill=F) +
    langcog::scale_fill_solarized() +
    theme_bw()+
    xlab('Model') +
    ylab("Posterior Probability Guessing") +
    ggtitle("Results of Latent Mixture Model")
```

The estimates are relatively stable. Most participants are not likely to be guessing. Four participants have a high probability of guessing across all models and are thus less likely to contribute to the linear regression. 30050 is more likely to be guessing in the models with the linear regressions.