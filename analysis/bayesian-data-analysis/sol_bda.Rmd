---
title: "SOL Bayesian Data Analysis"
author: "Kyle MacDonald"
date: "April 4, 2016"
output: html_document
bibliography: bda.bib
---

```{r, echo = F}
rm(list=ls()) # clear workspace
knitr::opts_chunk$set(
    fig.height=4, 
    fig.width=7)

set.seed(seed = 10) # so simulations are the same each time

library(langcog)
library(knitr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(polspline)
library(rethinking)
library(R2jags)
library(rjags)
library(binom)
library(lme4)
library(bootstrap)
library(magrittr)
library(stringr)
theme_set(theme_bw())

find_mode <- function(x) {
    ux <- unique(x)
    ux[which.max(tabulate(match(x, ux)))]
}

HDIofMCMC = function( sampleVec , credMass=0.95 ) {
    # Computes highest density interval from a sample of representative values,
    #   estimated as shortest credible interval.
    # Arguments:
    #   sampleVec
    #     is a vector of representative values from a probability distribution.
    #   credMass
    #     is a scalar between 0 and 1, indicating the mass within the credible
    #     interval that is to be estimated.
    # Value:
    #   HDIlim is a vector containing the limits of the HDI
    sortedPts = sort( sampleVec )
    ciIdxInc = ceiling( credMass * length( sortedPts ) )
    nCIs = length( sortedPts ) - ciIdxInc
    ciWidth = rep( 0 , nCIs )
    for ( i in 1:nCIs ) {
        ciWidth[ i ] = sortedPts[ i + ciIdxInc ] - sortedPts[ i ]
    }
    HDImin = sortedPts[ which.min( ciWidth ) ]
    HDImax = sortedPts[ which.min( ciWidth ) + ciIdxInc ]
    HDIlim = c( HDImin , HDImax )
    return( HDIlim )
}
```

In this document, you will find the details of our Bayesian analysis for the paper, "Real-time Language Comprehension in American Sign Language."

```{r}
# Load data
d <- read.csv("../../analysis/eye_movements/sol_ss_all.csv")

d %<>%  
    filter(age_group_collapsed == "Kids", value_cat == "Target") %>% 
    mutate(C_D_count = ifelse(is.na(C_D_count), 0, C_D_count),
           total_trials_shifting = C_T_count + C_D_count,
           Sub.Num = as.character(Sub.Num)) %>% 
    select(Sub.Num, age_peek_months, signs_produced, C_T_count, total_trials_shifting, 
           median_rt = median_ct_rt, mean_prop_looking_TD, age_group_collapsed)

# Standardize the data
d %<>% 
    mutate(age.s = (age_peek_months - mean(age_peek_months)) / sd(age_peek_months),
           acc.s = (mean_prop_looking_TD - mean(mean_prop_looking_TD)) / sd(mean_prop_looking_TD),
           rt.s = (median_rt - mean(median_rt)) / sd(median_rt))

d_voc <- d %>% 
    filter(is.na(signs_produced) == F) %>% 
    mutate(voc.s = (signs_produced - mean(signs_produced)) / sd(signs_produced))

```

```{r}
parameters = c( "zbeta0" , "zbeta1" , "zsigma" )
adaptSteps = 500  # Number of steps to "tune" the samplers
burnInSteps = 1000
nChains = 1 
thinSteps = 1
numSavedSteps = 50000
nIter = ceiling( ( numSavedSteps * thinSteps ) / nChains )
```

## Abstract

The ability to interpret language rapidly is critical for developing language proficiency. Research on real-time sentence processing by very young children has used eye movements as a window into their emerging comprehension abilities (Fernald & Marchman, 2012). In this study, we developed the first measures of children’s real-time comprehension of a visual language, American Sign Language (ASL). Participants were 29 native ASL-learning children (16-53 mos, 16 deaf and 13 hearing) and fluent adult signers (n=19). Children’s ASL comprehension improved with age, moving toward the efficiency of adult signers. Importantly, variation in children’s processing efficiency was associated with vocabulary size, linking the ability to establish reference in real time with language learning. Finally, both deaf and hearing ASL learners showed qualitatively similar patterns of looking behavior, suggesting that visual language processing is driven by experience with a visual language, and not by deafness. These findings show important parallels between children learning signed and spoken languages in the early development of real-time language comprehension.

## Why Bayesian stats?

We chose to use Bayesian analyses because it allowed us to include relevant prior knowledge about each participant in order to more accurately estimate the strength of the associations between RT on the VLP task and age/vocabulary. Specifically, the use of RT as a processing measure is based on the assumption that the timing of children’s first shifts are generated by the speed of lexical access, and not the result of random guessing. Thus, we created an analysis model where participants who were more likely to be guessers would have less of an influence on the estimated relations between RT and age/vocabulary.

To quantify each participant’s probability of guessing, we computed the proportion of signer-to-target (correct) and signer-to-distracter (incorrect) shifts for each child. Previous work using the Looking-While-Listening paradigm could not easily compute these values, since the task did not include a center fixation point. We then used a latent mixture model in which we assumed that the observed data (children’s initial shifts away from the signer) were generated by two processes (guessing and knowledge) that had different overall probabilities of success, with the “guessing group” having a probability of 50% and the “knowledge” group having a probability > 50%. The group membership of each participant was a latent variable inferred based on that participant’s proportion of correct signer-to-target shifts relative to the overall proportion of correct shifts across all participants (see Lee & Wagenmakers [2013] for a detailed discussion of this modeling approach). Each participant’s inferred group membership was used to weight their data proportional to our belief that they were guessing.  It is important to point out that we use this approach only in the analysis of RT because we think that “guessing behavior” is part of the underlying process of interest when measuring children’s accuracy on the VLP task. 

In all of the Bayesian linear models, we assume that each outcome variable (mean accuracies and RTs for each participant) is drawn from a Gaussian distribution with a mean, μ, and a standard deviation, σ. The mean is generated by a linear function consisting of an intercept term, α, which encodes the expected value of the outcome variable when the predictor is zero, and a slope term, β, which encodes the expected change in the outcome with each unit change in the predictor (i.e., the strength of association). We use weak priors for the intercept and the standard deviation, allowing the model to consider a wide range of plausible values. For the prior on the slope parameter, we use a weak Gaussian distribution truncated at zero, encoding our directional hypotheses for the relations between processing skills and age/vocabulary (i.e., that we predict that these relations should be null or improve with increasing age and larger vocabulary size). For each analysis, we present the following: a) the Bayes Factor (BF) comparing the likelihood of a linear model to a null model, b) the point estimates of α and β that maximize the posterior probability of the data, and c) the 95% Highest Density Interval (HDI) of each parameter’s posterior distribution, which provides information about the uncertainty of the estimate .

## Defining priors

RT estimate: is 33ms change per month in spoken language
Acc estimate is .0167 change per month in spoken language

## Model specifications

## Bayes Factor for model comparison (Savage-Dickey Method)

## Age as categorical variable

Models fit using STAN.

## Univariate regressions predicting processing measures 

All models fit using JAGS and based on models from Ch. 17 in @kruschke2014doing. 

First, we need to set the parameters for the MCMC simulations. This will be the same 
across all models. The key parameter of interest is the standardized slope: zbeta1.
This encodes the strength of the linear relationship between the predictor variable
and the outcome.

### Age and accuracy

First, we take samples from the prior predictive distribution to show that we are considering 
a plausible set of paraemter values for the slope and interecept. Here we are using our prior knowledge
from past work with spoken language showing that expected change in accuracy for each month of age is ~.0167, or 1.6% points. 

Draw samples from the prior.

```{r}
# to sample from prior we do not condition on the observed data
dataList_age_prior = list(
    #y = d$acc.s,
    x = d$age.s,
    Ntotal = length(d$age.s)
)

samples_prior <- jags(data = dataList_age_prior, parameters.to.save = parameters,
                      model.file="accuracy_model.txt", n.chains=nChains, 
                      n.iter=nIter, n.burnin = burnInSteps,
                      n.thin=1, DIC=F)

df_prior <- data.frame(zbeta0 = samples_prior$BUGSoutput$sims.list$zbeta0, 
                       zbeta1 = samples_prior$BUGSoutput$sims.list$zbeta1)

# transform standardized prior to measurement scale
df_prior %<>% 
    mutate( beta1 = zbeta1 * sd(d$mean_prop_looking_TD) / sd(d$age_peek_months),
            beta0 = zbeta0 * sd(d$mean_prop_looking_TD) + mean(d$mean_prop_looking_TD) - (zbeta1 * mean(d$age_peek_months) * sd(d$mean_prop_looking_TD)) / sd(d$age_peek_months)
    )
```

Now we can plot the prior distribution of the slope parameter (beta1) on the measurement scale. 

```{r}
ggplot() + geom_density(aes(df_prior$beta1)) +
    xlab("Beta 1") +
    ylab("Density") +
    ggtitle("Prior Predictive on Slope Parameter")
```

We can see that the prior model puts most of the density on a value of 0, or a null relationship between
age and accuracy. It is truncated at zero to eoncode our directional hypothesis: that the slope parameter
should be positive. And the prior does not consider extreme slope values beyond .04, or a 4% gain in accuracy with each month. 

Now that we are confident that our prior distribution produces reasonable hypotheses, we can 
condition on the data and quantify the association between children's age and accuracy on the VLP task. Conditioned on the data, we draw samples from the posterior.

```{r}
dataList_age = list(
    x = d$age.s,
    y = d$acc.s,
    Ntotal = length(d$age.s)
)

samples <- jags(data = dataList_age, parameters.to.save = parameters,
                model.file="accuracy_model.txt", n.chains=nChains, 
                n.iter=nIter, n.burnin = burnInSteps,
                n.thin=1, DIC=T)

df <- data.frame(zbeta0 = samples$BUGSoutput$sims.list$zbeta0, 
                 zbeta1 = samples$BUGSoutput$sims.list$zbeta1)

# transform standardized coefs to measurement scale coefs
df %<>% mutate(
    beta1 = zbeta1 * sd(d$mean_prop_looking_TD) / sd(d$age_peek_months),
    beta0 = zbeta0 * sd(d$mean_prop_looking_TD) + mean(d$mean_prop_looking_TD) - zbeta1 * mean(d$age_peek_months) * sd(d$mean_prop_looking_TD) / sd(d$age_peek_months)
)
```

The first question we ask is whether there is evidence of a linear relationship to measure. We answer this
question by using the Savage-Dickey method for estimating the Bayes Factor. This method uses the statistical fact that the Bayes factor is the ratio of the prior and posterior at the point in the parameter space that reduces the full model to the nested model.

Thus, we treat the null model of no association between age and accuracy as a special case of the
linear model where the slope parameter equals 0. 

```{r}
# Compute p(D) for posterior and prior: Savage-Dickey Method to get Bayes Factor
# Fits a density using splines to approx. log-density
# uses 1997 knot and deletion algorithm
fit.posterior <- logspline(df$zbeta1, lbound = 0)
posterior <- dlogspline(0, fit.posterior) # pdf @ beta=0

# get prior density using logspline
fit.prior <- logspline(df_prior$zbeta1, lbound = 0)
prior <- dlogspline(0, fit.prior) # pdf @ beta=0

ggplot(data=df) +
    geom_density(aes(x=zbeta1), linetype='dashed') +
    geom_density(aes(x=zbeta1), data = df_prior) +
    xlim(0,2) +
    xlab("Standardized Beta")
```

We get the Bayes Factor by taking the ratio of the density of the posterior and prior when the slope
parameter is zero. 

```{r}
# bayes factor
bf_acc_age <- posterior/prior
1/bf_acc_age
```

The prior and posterior are shown, and the posterior is about 12 times lower than the prior at the critical
point beta = 0. This means that the linear model is 12 times more likely than the null, intercept-only model. 

Now that we know there is evidence of a linear association, we can summarize the strength of the
association and the uncertainty around plausible values for slope parameter.

```{r}
post_mode_b1_acc_age <- round(find_mode(df$beta1), 3)
post_mode_b0_acc_age <- round(find_mode(df$beta0), 3)
HDI_b1_acc_age <- HDIofMCMC(df$beta1 , credMass = 0.95 )

## simulate posterior distribution of mu for each value of age to get HDI for MAP regression line
mu.link <- function(x) {df$beta0 + df$beta1*x}
age.seq <- seq(from = 10, to = 60, by = 1)
mu <- sapply(age.seq, mu.link)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.95)
mu.HPDI_tidy <- data.frame(age_peek_months = age.seq, 
                           hpdi_lower = mu.HPDI[1,], hpdi_upper = mu.HPDI[2,],
                           mu.mean = mu.mean)

ggplot(data = d) +
    geom_line(aes(x = age_peek_months, y = mu.mean), data = mu.HPDI_tidy, size = 2) +
    geom_point(aes(age_peek_months, mean_prop_looking_TD), color = "black", size = 5.5) +
    geom_point(aes(age_peek_months, mean_prop_looking_TD), color = "grey50", size = 4) +
    geom_ribbon(aes(x = age_peek_months, ymin = hpdi_lower, ymax = hpdi_upper), data = mu.HPDI_tidy,
                alpha = 0.2) +
    ylab("Accuracy") +
    xlab("Child's Age (months)") +
    coord_cartesian(xlim=c(15, 55), ylim=c(0.25, 0.95)) +
    ggtitle(bquote(list(beta==.(as.character(round(post_mode_b1_acc_age, 3))),
                        HDI=.(paste("95% HDI [", round(HDI_b1_acc_age[1],3) , "," , 
                                     round(HDI_b1_acc_age[2], 3), "]"))))) +
    theme(plot.title = element_text(size = 20),
        axis.title.x = element_text(colour="grey30",size=22,
                                      angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.title.y = element_text(colour="grey30",size=22,
                                      hjust=0.5,vjust=0.5,face="plain"),
          axis.text.x = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.text.y = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          panel.grid.major=element_blank())
```

### Vocab and accuracy 

Draw samples from the prior.

```{r}
dataList_voc_prior = list(
    #y = d_voc$mean_prop_looking_TD,
    x = d_voc$voc.s,
    Ntotal = length(d_voc$age.s)
)

samples_prior <- jags(data = dataList_voc_prior, parameters.to.save = parameters,
                      model.file="accuracy_model.txt", n.chains=nChains, 
                      n.iter=nIter, n.burnin = burnInSteps,
                      n.thin=1, DIC=F)

df_prior <- data.frame(zbeta0 = samples_prior$BUGSoutput$sims.list$zbeta0, 
                       zbeta1 = samples_prior$BUGSoutput$sims.list$zbeta1)

# transform standardized prior to measurement scale
df_prior %<>% 
    mutate( beta1 = zbeta1 * sd(d_voc$mean_prop_looking_TD) / sd(d_voc$signs_produced),
            beta0 = zbeta0 * sd(d_voc$mean_prop_looking_TD) + mean(d_voc$signs_produced) - (zbeta1 * mean(d_voc$signs_produced) * sd(d_voc$mean_prop_looking_TD)) / sd(d_voc$signs_produced)
    )
```

Draw samples from the posterior.

```{r}
dataList_voc = list(
    x = d_voc$voc.s,
    y = d_voc$acc.s,
    Ntotal = length(d_voc$age.s)
    
)
samples <- jags(data = dataList_voc, parameters.to.save = parameters,
                model.file="accuracy_model.txt", n.chains=nChains, n.iter=nIter, 
                n.thin=1, DIC=T, n.burnin=burnInSteps)

df <- data.frame(zbeta0 = samples$BUGSoutput$sims.list$zbeta0, 
                 zbeta1 = samples$BUGSoutput$sims.list$zbeta1)

# transform standardized prior to measurement scale
df %<>%
    mutate( beta1 = zbeta1 * sd(d_voc$mean_prop_looking_TD) / sd(d_voc$signs_produced),
            beta0 = zbeta0 * sd(d_voc$mean_prop_looking_TD) + mean(d_voc$mean_prop_looking_TD) - zbeta1 * mean(d_voc$signs_produced) * sd(d_voc$mean_prop_looking_TD) / sd(d_voc$signs_produced)
    )
```

We get the Bayes Factor by taking the ratio of the density of the posterior and prior when the slope
parameter is zero. 

```{r}
fit.posterior <- logspline(df$beta1, lbound = 0)
posterior <- dlogspline(0, fit.posterior) # pdf @ beta=0

# get prior density using logspline
fit.prior <- logspline(df_prior$beta1, lbound = 0)
prior <- dlogspline(0, fit.prior) # pdf @ beta=0

ggplot(data=df) +
    geom_density(aes(x=zbeta1), linetype='dashed') +
    geom_density(aes(x=zbeta1), data = df_prior) +
    xlim(0,2) +
    xlab("Standardized Beta")
```

```{r}
bf_acc_voc <- posterior/prior # bayes factor
1/bf_acc_voc
```

Now that we know there is evidence of a linear association, we can summarize the strength of the
association and the uncertainty around plausible values for slope parameter.

```{r}
## get MAP and HDI on slope and intercept
post_mode_b1_accvoc <- round(find_mode(df$beta1), 3)
post_mode_b0_accvoc <- round(find_mode(df$beta0), 3)
HDI_b1_accvoc <- HDIofMCMC(df$beta1 , credMass = 0.95 )

## simulate to get HDI for regression line
vocab.seq <- seq(from = 15, to = 85, by = 1)
mu.link <- function(x) {df$beta0 + df$beta1*x}
mu <- sapply(vocab.seq, mu.link)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.95)
mu.HPDI_tidy <- data.frame(signs_produced = vocab.seq, hpdi_lower = mu.HPDI[1,], hpdi_upper = mu.HPDI[2,])

ggplot(data = d_voc) +
    geom_line(aes(x = signs_produced, y = mu.mean), data = mu.HPDI_tidy, size = 2) +
    geom_point(aes(signs_produced, mean_prop_looking_TD), color = "black", size = 5.5) +
    geom_point(aes(signs_produced, mean_prop_looking_TD), color = "grey50", size = 4) +
    geom_ribbon(aes(x = signs_produced, ymin = hpdi_lower, ymax = hpdi_upper), data = mu.HPDI_tidy,
                alpha = 0.2) +
    ylab("Accuracy") +
    xlab("Signs Produced") +
    coord_cartesian(xlim=c(18, 82), ylim=c(0.25, 0.95)) +
    ggtitle(bquote(list(beta==.(as.character(round(post_mode_b1_accvoc, 3))),
                        HDI=.(paste("95% HDI [", round(HDI_b1_accvoc[1],3) , "," , 
                                    round(HDI_b1_accvoc[2], 3), "]"))))) +
    theme(plot.title = element_text(size = 20),
        axis.title.x = element_text(colour="grey30",size=22,
                                      angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.title.y = element_text(colour="grey30",size=22,
                                      hjust=0.5,vjust=0.5,face="plain"),
          axis.text.x = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.text.y = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          panel.grid.major=element_blank())
```

### Age and RT 

Draw samples from the prior.

```{r}
# Specify the data in a list, for later shipment to JAGS:
dataList_prior = list(
    x = d$age.s,
    #y = d$rt.s,
    n_correct = d$C_T_count,
    n_trials = d$total_trials_shifting,
    Ntotal = length(d$acc.s)
)

parameters = c( "true_beta0" , "true_beta1" )

myinits <-  list(list(phi = 0.75, z = round(runif(length(d$total_trials_shifting)))))

samples_prior <- jags(data = dataList_prior, parameters.to.save = parameters,
                model.file="rt_model.txt", n.chains=nChains, 
                n.iter=nIter, n.burnin = burnInSteps, inits = myinits,
                n.thin=1, DIC=T)


df_rt_age_prior <- data.frame(zbeta0 = samples_prior$BUGSoutput$sims.list$true_beta0,
                        zbeta1 = samples_prior$BUGSoutput$sims.list$true_beta1)

## transform to measurement scale (RT in ms)
df_rt_age_prior %<>% mutate(
    beta1 = zbeta1 * sd(d$median_rt) / sd(d$age_peek_months),
    beta0 = zbeta0 * sd(d$median_rt) + mean(d$median_rt) - zbeta1 * mean(d$age_peek_months) * sd(d$median_rt) / sd(d$age_peek_months)
)
```

First, we take samples from the prior predictive distribution to show that we are considering 
a plausible set of parameter values for the slope and interecept. Here we are using our prior knowledge
from past work with spoken language showing that expected change in RT for each month of age is ~33ms.

```{r}
ggplot() + geom_density(aes(df_rt_age_prior$beta1)) +
    xlab("Beta 1") +
    ylab("Density") +
    ggtitle("Prior Predictive on Slope Parameter")
```

By inspecting the prior distribution we can see that the model makes reasonable predictions for the slope parameter. The distribution is centered around zero, conservatively predicting no association between age 
and RT. The value from prior data of 33 ms is considered plausible, and extends to extreme values of triple that value.

Draw samples from posterior. 

```{r}
dataList_post = list(
    x = d$age.s,
    y = d$rt.s,
    n_correct = d$C_T_count,
    n_trials = d$total_trials_shifting,
    Ntotal = length(d$acc.s)
)

parameters = c( "true_beta0" , "true_beta1" , "phi" , "z")

myinits <-  list(list(phi = 0.75, z = round(runif(length(d$total_trials_shifting)))))

samples_post <- jags(data = dataList_post, parameters.to.save = parameters,
                model.file="rt_model.txt", n.chains=nChains, inits = myinits,
                n.iter=nIter, n.burnin = burnInSteps,
                n.thin=1, DIC=T)

df_rt_age_post <- data.frame(zbeta0 = samples_post$BUGSoutput$sims.list$true_beta0,
                        zbeta1 = samples_post$BUGSoutput$sims.list$true_beta1)

## transform to measurement scale (RT in ms)
df_rt_age_post %<>% mutate(
    beta1 = zbeta1 * sd(d$median_rt) / sd(d$age_peek_months),
    beta0 = zbeta0 * sd(d$median_rt) + mean(d$median_rt) - zbeta1 * mean(d$age_peek_months) * sd(d$median_rt) / sd(d$age_peek_months)
)
```

Plot prior with posterior.

```{r}
ggplot(data=df_rt_age_post) +
    geom_density(aes(x=zbeta1), linetype='dashed') +
    geom_density(aes(x=zbeta1), data = df_rt_age_prior) +
    xlim(-1, 0) +
    xlab("Standardized Beta")
```

Get Bayes Factor using Savage-Dickey method.

```{r}
# Compute p(D) for posterior and prior: Savage-Dickey Method to get Bayes Factor
# Fits a density using splines to approx. log-density
# uses 1997 knot and deletion algorithm
fit.posterior <- logspline(df_rt_age_post$zbeta1, ubound = 0)
posterior <- dlogspline(0, fit.posterior) # pdf @ beta=0

# get prior density using logspline
fit.prior <- logspline(df_rt_age_prior$zbeta1, ubound = 0)
prior <- dlogspline(0, fit.prior) # pdf @ beta=0

bf_rt_age <- posterior / prior
1/bf_rt_age
```

The data is 1.14 times as likely under the linear model. This is the weakest evidence among all four linear
associations. 

We can still estimate the slope parameter, despite the weak Bayes Factor.

```{r}
HDI_rt_age_b1 <- HDIofMCMC(df_rt_age_post$beta1 , credMass = 0.95 )
post_rt_age_mode_b1 <- round(find_mode(df_rt_age_post$beta1), 3)
post_rt_age_mode_b0 <- round(find_mode(df_rt_age_post$beta0), 3)

# simulate to get HDI around MAP regression line
mu.link <- function(x) {df_rt_age_post$beta0 + df_rt_age_post$beta1*x}
age.seq <- seq(from = 10, to = 60, by = 1)
mu <- sapply(age.seq, mu.link)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.95)
mu.HPDI_tidy <- data.frame(age_peek_months = age.seq, 
                           hpdi_lower = mu.HPDI[1,], hpdi_upper = mu.HPDI[2,],
                           mu.rt = mu.mean)

ggplot(data = d) +
    geom_line(aes(x = age_peek_months, y = mu.rt), data = mu.HPDI_tidy, size = 2) +
    geom_point(aes(age_peek_months, median_rt), color = "black", size = 5.5) +
    geom_point(aes(age_peek_months, median_rt), color = "grey50", size = 4) +
    geom_ribbon(aes(x = age_peek_months, ymin = hpdi_lower, ymax = hpdi_upper), data = mu.HPDI_tidy,
                alpha = 0.2) +
    ylab("Reaction Time (ms)") +
    xlab("Child's Age (months)") +
    coord_cartesian(xlim=c(15, 55), ylim=c(700, 2000)) +
    ggtitle(bquote(list(beta==.(as.character(round(post_rt_age_mode_b1, 2))),
                        HDI=.(paste("95% HDI [", round(HDI_rt_age_b1[1],3) , "," , 
                                    round(HDI_rt_age_b1[2], 3), "]"))))) +
    theme(plot.title = element_text(size = 18),
        axis.title.x = element_text(colour="grey30",size=22,
                                      angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.title.y = element_text(colour="grey30",size=22,
                                      hjust=0.5,vjust=0.5,face="plain"),
          axis.text.x = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.text.y = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          panel.grid.major=element_blank())
```

### Vocab and RT 

Draw samples from prior.

```{r}
dataList_prior = list(
    x = d_voc$voc.s,
    #y = d_voc$rt.s,
    n_correct = d_voc$C_T_count,
    n_trials = d_voc$total_trials_shifting,
    Ntotal = length(d_voc$voc.s)
)

parameters = c( "true_beta0" , "true_beta1" )

myinits <-  list(list(phi = 0.75, z = round(runif(length(d_voc$total_trials_shifting)))))

samples_prior <- jags(data = dataList_prior, parameters.to.save = parameters,
                model.file="rt_model.txt", n.chains=nChains, inits = myinits,
                n.iter=nIter, n.burnin = burnInSteps,
                n.thin=1, DIC=F)


df_rt_voc_prior <- data.frame(zbeta0 = samples_prior$BUGSoutput$sims.list$true_beta0,
                        zbeta1 = samples_prior$BUGSoutput$sims.list$true_beta1)

## transform to measurement scale (RT in ms)
df_rt_voc_prior %<>% mutate(
    beta1 = zbeta1 * sd(d_voc$median_rt) / sd(d_voc$signs_produced),
    beta0 = zbeta0 * sd(d_voc$median_rt) + mean(d_voc$median_rt) - zbeta1 * mean(d_voc$signs_produced) * sd(d_voc$median_rt) / sd(d_voc$signs_produced)
)
```

```{r}
ggplot() + geom_density(aes(df_rt_voc_prior$beta1)) +
    xlab("Beta 1") +
    ylab("Density") +
    ggtitle("Prior Predictive on Slope Parameter")
```

Draw samples from posterior.

```{r}
dataList_post = list(
    x = d_voc$voc.s,
    y = d_voc$rt.s,
    n_correct = d_voc$C_T_count,
    n_trials = d_voc$total_trials_shifting,
    Ntotal = length(d_voc$voc.s)
)

samples_post <- jags(data = dataList_post, parameters.to.save = parameters,
                model.file="rt_model.txt", n.chains=nChains, inits = myinits,
                n.iter=nIter, n.burnin = burnInSteps,
                n.thin=1, DIC=F)


df_rt_voc_post <- data.frame(zbeta0 = samples_post$BUGSoutput$sims.list$true_beta0,
                        zbeta1 = samples_post$BUGSoutput$sims.list$true_beta1)

## transform to measurement scale (RT in ms)
df_rt_voc_post %<>% mutate(
    beta1 = zbeta1 * sd(d_voc$median_rt) / sd(d_voc$signs_produced),
    beta0 = zbeta0 * sd(d_voc$median_rt) + mean(d_voc$median_rt) - zbeta1 * mean(d_voc$signs_produced) * sd(d_voc$median_rt) / sd(d_voc$signs_produced)
)
```

Plot prior with posterior.

```{r}
ggplot(data=df_rt_voc_post) +
    geom_density(aes(x=zbeta1), linetype='dashed') +
    geom_density(aes(x=zbeta1), data = df_rt_voc_prior) +
    xlim(-2, 0) +
    xlab("Standardized Beta")
```

Get Bayes Factor.

```{r}
# Compute p(D) for posterior and prior: Savage-Dickey Method to get Bayes Factor
# Fits a density using splines to approx. log-density
# uses 1997 knot and deletion algorithm
fit.posterior <- logspline(df_rt_voc_post$zbeta1, ubound = 0)
posterior <- dlogspline(0, fit.posterior) # pdf @ beta=0

# get prior density using logspline
fit.prior <- logspline(df_rt_voc_prior$zbeta1, ubound = 0)
prior <- dlogspline(0, fit.prior) # pdf @ beta=0

bf_rt_voc <- posterior / prior
1/bf_rt_voc
```

Estimate strength of association.

```{r}
HDI_rt_voc_b1 <- HDIofMCMC( df_rt_voc_post$beta1 , credMass = 0.95 )
post_mode_b1_rt_voc <- round(find_mode(df_rt_voc_post$beta1), 3)
post_mode_b0_rt_voc <- round(find_mode(df_rt_voc_post$beta0), 3)


vocab.seq <- seq(from = 15, to = 85, by = 1)
mu.link <- function(x) {df_rt_voc_post$beta0 + df_rt_voc_post$beta1*x}
mu <- sapply(vocab.seq, mu.link)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.95)
mu.HPDI_tidy <- data.frame(signs_produced = vocab.seq, hpdi_lower = mu.HPDI[1,], hpdi_upper = mu.HPDI[2,],
                           mu.rt = mu.mean)

# plot
ggplot(data = d_voc) +
    geom_line(aes(x = signs_produced, y = mu.mean), data = mu.HPDI_tidy, size = 2) +
    geom_point(aes(signs_produced, median_rt), color = "black", size = 5.5) +
    geom_point(aes(signs_produced, median_rt), color = "grey50", size = 4) +
    geom_ribbon(aes(x = signs_produced, ymin = hpdi_lower, ymax = hpdi_upper), data = mu.HPDI_tidy,
                alpha = 0.2) +
    ylab("Reaction Time (ms)") +
    xlab("Signs Produced") +
    coord_cartesian(xlim=c(18, 82), ylim=c(700, 2000)) +
   ggtitle(bquote(list(beta==.(as.character(round(post_mode_b1_rt_voc, 2))),
                        HDI=.(paste("95% HDI [", round(HDI_rt_voc_b1[1],3) , "," , 
                                     round(HDI_rt_voc_b1[2], 3), "]")))))  +
    theme(plot.title = element_text(size=20),
        axis.title.x = element_text(colour="grey30",size=20,
                                      angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.title.y = element_text(colour="grey30",size=22,
                                      hjust=0.5,vjust=0.5,face="plain"),
          axis.text.x = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          axis.text.y = element_text(colour="grey20",size=18,
                                     angle=0,hjust=0.5,vjust=0,face="plain"),
          panel.grid.major=element_blank())

```

### Table of model summaries


```{r}
models <- c("Acc ~ Age", "Acc ~ Voc", "RT ~ Age", "RT ~ Voc")
bfs <- c(1/bf_acc_age, 1/bf_acc_voc, 1/bf_rt_age, 1/bf_rt_voc)
bmaps <- c(post_mode_b1_acc_age, post_mode_b1_accvoc, post_rt_age_mode_b1, post_mode_b1_rt_voc)
hdi_lower <- c(HDI_b1_acc_age[1], HDI_b1_accvoc[1], HDI_rt_age_b1[1], HDI_rt_voc_b1[1])
hdi_upper <- c(HDI_b1_acc_age[2], HDI_b1_accvoc[2], HDI_rt_age_b1[2], HDI_rt_voc_b1[2])

table1 <- data.frame(models, bfs, bmaps, hdi_lower, hdi_upper)

knitr::kable(table1)
```